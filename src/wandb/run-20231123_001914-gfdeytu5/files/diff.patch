diff --git a/src/actor_critic.pt b/src/actor_critic.pt
index c66696b..7096862 100644
Binary files a/src/actor_critic.pt and b/src/actor_critic.pt differ
diff --git a/src/actor_critic.py b/src/actor_critic.py
deleted file mode 100644
index 3e32602..0000000
--- a/src/actor_critic.py
+++ /dev/null
@@ -1,68 +0,0 @@
-import torch
-from torch import nn, tensor
-from nets import discrete_net, continuous_net, critic
-from torch.distributions import Normal, Categorical
-import numpy as np
-
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-# an implementation of the actor-critic 
-class actor_critic(nn.Module):
-	def __init__(self, state_dim:int, action_dim:int, 
-		hidden_dim:int, num_layers:int, 
-		dropout:int, continuous:bool) -> None:
-		super(actor_critic, self).__init__()
-		self.state_dim = state_dim
-		self.action_dim = action_dim
-		self.hidden_dim = hidden_dim
-		self.continuous = continuous
-		self.num_layers = num_layers
-		self.dropout = dropout
-		if(continuous):
-			self.actor = continuous_net(hidden_dim, state_dim, action_dim, num_layers, dropout)
-			self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(action_dim)))
-		else:
-			self.actor = discrete_net(hidden_dim, state_dim, action_dim, num_layers, dropout)
-		self.critic = critic(hidden_dim, state_dim, num_layers, dropout) 
-		
-	def forward(self):
-		pass
-
-	# set the new action std. We will also set a new action variance
-	def set_action_std(self, new_action_std:float):
-		self.action_std_init = new_action_std
-		self.action_var = torch.full((self.action_dim, ), self.action_std_init ** 2)
-
-	def act(self, state):
-		if(self.continuous):
-			action_mean = self.actor(state)
-			action_logstd = self.actor_logstd.expand_as(action_mean)
-			action_std = torch.exp(action_logstd)
-			probs = Normal(action_mean, action_std)
-			action = probs.sample()
-		else:
-			probs = self.actor(state)
-			dist = Categorical(logits=probs)
-			action = dist.sample()
-		# running the policy to produce values for replay buffer. Can detach.
-		return action.detach().cpu(), dist.log_prob(action).detach().cpu(), self.critic(state).detach().cpu()
-
-	def value(self, state):
-		return self.critic(state)
-
-	def evaluate(self, state, action=None):
-		if(self.continuous):
-			action_mean = self.actor(state)
-			action_logstd = self.actor_logstd.expand_as(action_mean)
-			dist = Normal(action_mean, action_logstd)
-			action = dist.rsample()
-		else:
-			logits = self.actor(state)
-			dist = Categorical(logits=logits)
-			if(action is None):
-				action = dist.sample()
-		#log_prob = dist.log_prob(action)	
-		entropy = dist.entropy()
-		return action, dist.log_prob(action), dist.entropy(), self.critic(state)
diff --git a/src/nets/nets.py b/src/nets/nets.py
index 86d72e7..0d52bab 100644
--- a/src/nets/nets.py
+++ b/src/nets/nets.py
@@ -5,8 +5,6 @@ from torch.distributions import Categorical, Normal, MultivariateNormal
 import torch.nn.functional as F
 import numpy as np
 
-
-
 # orthogonal initialization
 def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
         torch.nn.init.orthogonal_(layer.weight, std)
@@ -21,13 +19,10 @@ class discrete_net(nn.Module):
         for _ in range(num_layers - 1):
             layers.extend([layer_init(nn.Linear(dim, dim)), nn.Tanh(), nn.Dropout(dropout)])
         layers.append(layer_init(nn.Linear(dim, output_dim), action_std))
-        #layers.append(nn.Softmax(dim=0))
         self.net = nn.Sequential(*layers)
-    
     def forward(self, input:tensor) -> tensor:
         return self.net(input)
 
-
 class continuous_net(nn.Module):
     def __init__(self, dim:int, input_dim:int, output_dim:int, num_layers:int, dropout:float, action_std=0.01) -> None:
         super().__init__()
diff --git a/src/ppo.py b/src/ppo.py
index 1f5bf15..799270b 100644
--- a/src/ppo.py
+++ b/src/ppo.py
@@ -4,16 +4,18 @@ from nets import discrete_net, continuous_net, critic
 from torch.distributions import MultivariateNormal, Categorical
 import gym
 import time
-from actor_critic import actor_critic
+from models import actor_critic
 import numpy as np
 import matplotlib.pyplot as plt
 from tqdm import tqdm
 import sys, os, time
 from torch.utils.tensorboard import SummaryWriter
+from tqdm import tqdm
 
 device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
-device = torch.device('cpu')
+#device = torch.device('cpu')
 
+#print(device)
 class torch_buffer():
 	def __init__(self, observation_shape, action_shape, num_steps, num_envs):
 		self.observation_shape = observation_shape
@@ -37,6 +39,9 @@ class torch_buffer():
 
 # implement their plotting strategies
 # weights and biases graphs
+
+# make an abstract algorithm class
+# to generalize to different strategies
 class ppo():
 	# add the metrics
 	def __init__(self, params):
@@ -58,20 +63,21 @@ class ppo():
 		self.num_updates = self.total_timesteps // self.all_steps
 		self.run_name = f"{self.gym_id}__{self.exp_name}__{self.seed}__{int(time.time())}"
 		self.envs = gym.vector.SyncVectorEnv(
-	    	[self.make_env(self.gym_id, int(self.seed) + i, i, params['capture_video']) for i in range(self.num_envs)]
+	    	[self.make_env(self.gym_id, i, params['capture_video']) for i in range(self.num_envs)]
 	    )
+		#self.envs = gym.vector.make(self.gym_id, num_envs=self.num_envs)
 		self.state_dim = self.envs.single_observation_space.shape
 		if(self.continuous):
 			self.action_dim = self.envs.single_action_space.shape
 		else:
 			self.action_dim = self.envs.single_action_space.n
-		#print(self.env.single_action_space.shape)	
+
 		self.policy = actor_critic(self.state_dim[0], 
-			self.action_dim, self.hidden_dim, self.num_layers, self.dropout, self.continuous)
+			self.action_dim, self.hidden_dim, self.num_layers, self.dropout, self.continuous).to(device)
 
 		# just keep them separate here
 		self.policy_old = actor_critic(self.state_dim[0], 
-			self.action_dim, self.hidden_dim, self.num_layers, self.dropout, self.continuous)
+			self.action_dim, self.hidden_dim, self.num_layers, self.dropout, self.continuous).to(device)
 		self.policy_old.load_state_dict(self.policy.state_dict())
 
 		self.buffer = torch_buffer(self.state_dim, self.envs.single_action_space.shape, self.num_steps, self.num_envs)
@@ -80,7 +86,7 @@ class ppo():
 		self.total_episode_lengths = []
 		self.x_indices = []
 
-	def make_env(self, gym_id, seed, idx, capture_video):
+	def make_env(self, gym_id, idx, capture_video):
 		def thunk():
 			env = gym.make(gym_id)
 			env = gym.wrappers.RecordEpisodeStatistics(env)
@@ -93,13 +99,11 @@ class ppo():
 				env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
 				env = gym.wrappers.NormalizeReward(env)
 				env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-			env.seed(seed)
-			env.action_space.seed(seed)
-			env.observation_space.seed(seed)
 			return env
 		return thunk
 
     # do the O(1) accesses slow down the code to a significant degree
+	#@profile
 	def rewards_to_go(self, step, next_obs, global_step, writer):
 		with torch.no_grad():
 			#action, logprob, _, value= self.agent.get_action_and_value(next_obs)
@@ -107,69 +111,77 @@ class ppo():
 			self.buffer.values[step] = value.flatten()
 			self.buffer.actions[step] = action
 			self.buffer.log_probs[step] = logprob
-			next_obs, reward, done, info = self.envs.step(action.cpu().numpy())
-			for item in info:
-				if "episode" in item.keys():
-					self.total_returns.append(item["episode"]["r"])
-					self.total_episode_lengths.append(item["episode"]["l"])
-					self.x_indices.append(global_step)
-					writer.add_scalar("charts/episodic_return", item["episode"]["r"], global_step)
-					writer.add_scalar("charts/episodic_length", item["episode"]["l"], global_step)
-			self.buffer.rewards[step] = torch.tensor(reward).to(device).view(-1)
+			next_obs, reward, done, _, info = self.envs.step(action.cpu().numpy())
+			if('final_info' in info.keys()):
+				for item in info['final_info']:
+					if item is not None:
+						self.total_returns.append(item["episode"]["r"])
+						self.total_episode_lengths.append(item["episode"]["l"])
+						self.x_indices.append(global_step)
+						writer.add_scalar("charts/episodic_return", item["episode"]["r"], global_step=global_step)
+						writer.add_scalar("charts/episodic_length", item["episode"]["l"], global_step=global_step)
+			self.buffer.rewards[step] = torch.tensor(reward).view(-1)
 			# torch.Tensor outputs a Float tensor, while tensor.tensor infers a dtype
 			next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)
+			#sys.exit()
 		return next_obs, next_done
 
+	def run_gae(self, next_value, next_done):
+		advantages = torch.zeros_like(self.buffer.rewards)
+		lastgaelam = 0
+		for t in reversed(range(self.num_steps)):
+			if t == self.num_steps - 1:
+				nextnonterminal = 1.0 - next_done
+				nextvalues = next_value
+			else:
+				nextnonterminal = 1.0 - self.buffer.terminals[t + 1]
+				nextvalues = self.buffer.values[t + 1]
+			# If episode finishes after timestep t, we mask the value and the previous advantage value, 
+			# so that advantages[t] stores the (reward - value) at that timestep without
+			# taking a value from the next episode, or storing a value in this episode to be multiplied
+			# in the calculation of the rollout of the next episode
+			delta = self.buffer.rewards[t] + self.gamma * nextvalues * nextnonterminal - self.buffer.values[t]
+			advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam
+		returns = advantages + self.buffer.values
+		return returns, advantages
+
+	# these can be stored in a separate file
+	def normal_advantage(self, next_value, next_done):
+		# the default rollout scheme
+		returns = torch.zeros_like(self.buffer.rewards).to(device)
+		for t in reversed(range(self.num_steps)):
+			if t == self.num_steps - 1:
+				nextnonterminal = 1.0 - next_done
+				next_return = next_value
+			else:
+				nextnonterminal = 1.0 - self.buffer.terminals[t + 1]
+				next_return = returns[t + 1]
+			returns[t] = self.buffer.rewards[t] + self.gamma * nextnonterminal * next_return
+		advantages = returns - self.buffer.values
+		return returns, advantages
+
 	def advantages(self, next_obs, next_done):
 		with torch.no_grad():
-			next_value = self.policy_old.critic(next_obs).reshape(1, -1)
-            # generalized advantage estimation
+			#next_value = self.policy_old.critic(next_obs).reshape(1, -1)
+			next_value = self.policy_old.value(next_obs)
 			if self.gae:
-				advantages = torch.zeros_like(self.buffer.rewards).to(device)
-				lastgaelam = 0
-				for t in reversed(range(self.num_steps)):
-					if t == self.num_steps - 1:
-						nextnonterminal = 1.0 - next_done
-						# bootstrapped from previous values
-						nextvalues = next_value
-					else:
-						# if some time step is terminal, we create a mask
-						nextnonterminal = 1.0 - self.buffer.terminals[t + 1]
-						nextvalues = self.buffer.values[t + 1]
-					# nextnonterminal is a mask
-					# delta for our td difference 
-					delta = self.buffer.rewards[t] + self.gamma * nextvalues * nextnonterminal - self.buffer.values[t]
-					advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam
-				returns = advantages + self.buffer.values
+				returns, advantages = self.run_gae(next_value, next_done)
 			else:
-				returns = torch.zeros_like(self.buffer.rewards).to(device)
-				for t in reversed(range(self.num_steps)):
-					if t == self.num_steps - 1:
-						nextnonterminal = 1.0 - next_done
-						next_return = next_value
-					else:
-						nextnonterminal = 1.0 - self.buffer.terminals[t + 1]
-						next_return = returns[t + 1]
-					returns[t] = self.buffer.rewards[t] + self.gamma * nextnonterminal * next_return
-				advantages = returns - self.buffer.values
+				returns, advantages = self.run_normal_advantage(next_value, next_done)
 		return returns, advantages
 
+	#@profile
 	def train(self):
 		if self.track:
 			import wandb
 			wandb.init(project='ppo',entity='Aurelian',sync_tensorboard=True,config=None,name=self.run_name,monitor_gym=True,save_code=True)
 		writer = SummaryWriter(f"runs/{self.run_name}")
-		#writer.add_text(
-		#    "hyperparameters",
-		#    "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
-		#)
-
 		global_step = 0
 		start_time = time.time()
-		next_obs = torch.Tensor(self.envs.reset()).to(device)
+		next_obs = torch.Tensor(self.envs.reset(seed=list(range(self.num_envs)))[0]).to(device)
 		next_done = torch.zeros(self.num_envs).to(device)
 		policy_losses = []
-		for update in range(1, self.num_updates + 1):
+		for update in tqdm(range(1, self.num_updates + 1)):
 			# adjust learning rate
 			if self.anneal_lr:
 				frac = 1.0 - (update - 1.0) / self.num_updates
@@ -181,8 +193,10 @@ class ppo():
 				global_step += 1 * self.num_envs
 				self.buffer.states[step] = next_obs
 				self.buffer.terminals[step] = next_done
-				next_obs, next_done = self.rewards_to_go(step, next_obs, global_step, writer)		
+				next_obs, next_done = self.rewards_to_go(step, next_obs, global_step, writer)	
+
 			returns, advantages = self.advantages(next_obs, next_done)
+
 			(b_obs, b_logprobs, b_actions, 
 				b_advantages, b_returns, b_values) = self.buffer.flatten(returns, advantages)
 			b_inds = np.arange(self.all_steps)
@@ -228,6 +242,8 @@ class ppo():
 			var_y = np.var(y_true)
 			explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
 
+			# should be done in a separate function
+			# Writer object?
 			writer.add_scalar("charts/learning_rate", self.optimizer.param_groups[0]["lr"], global_step)
 			writer.add_scalar("losses/value_loss", value_loss.item(), global_step)
 			writer.add_scalar("losses/policy_loss", policy_loss.item(), global_step)
@@ -240,15 +256,15 @@ class ppo():
 			writer.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
 		self.envs.close()
 		writer.close()
+		torch.save(self.policy, 'actor_critic_' + str(self.num_layers) + '.pt')
+		self.plot_episodic_returns(np.array(self.total_returns), np.array(np.array(self.x_indices)), 'episodic returns')
+		self.plot_episodic_returns(np.array(self.total_episode_lengths), np.array(np.array(self.x_indices)), 'episodic lengths')
 
-		torch.save(self.policy, 'actor_critic.pt')
-		self.plot_episodic_returns(np.array(self.total_returns), np.array(np.array(self.x_indices)))
-		self.plot_episodic_returns(np.array(self.total_episode_lengths), np.array(np.array(self.x_indices)))
+		return self.total_returns, self.total_episode_lengths, self.x_indices
 
 		#self.plot_episodic_returns(np.array(policy_losses), np.arange(len(policy_losses)))
 
 	def plot(self, loss, x_indices):
-		print(loss.shape)
 		#timesteps = np.arange(1, loss.shape[0] + 1)
 		plt.plot(np.array(x_indices), loss)
 		plt.xlabel('Timestep')
@@ -259,9 +275,8 @@ class ppo():
 	def moving_average(self, data, window_size):
 		return np.convolve(data, np.ones(window_size)/window_size, mode='valid')
 
-	def plot_episodic_returns(self, episodic_returns, x_indices, window_size=10):
-		print(len(episodic_returns))
-		print(len(x_indices))
+	# add chart titles and all of that stuff
+	def plot_episodic_returns(self, episodic_returns, x_indices, title, window_size=10):
 		smoothed_returns = self.moving_average(episodic_returns, window_size)
 		plt.plot(x_indices, episodic_returns, label='Episodic Returns')
 		plt.plot(x_indices[9:], smoothed_returns, label=f'Moving Average (Window Size = {window_size})', color='red')
@@ -269,4 +284,4 @@ class ppo():
 		plt.xlabel('Timestep')
 		plt.ylabel('Return')
 		plt.legend()
-		plt.show()
+		plt.savefig(title + '_num_layers_' + str(self.num_layers) + '_dropout_' + str(self.dropout) + '_num_envs_' + str(self.num_envs) + '_num_mb_' + str(self.num_minibatches) + '.png')
diff --git a/src/run.py b/src/run.py
index 2b9d196..ec2847f 100644
--- a/src/run.py
+++ b/src/run.py
@@ -2,6 +2,26 @@
 import torch
 from ppo import ppo
 import os, sys, argparse, time
+import matplotlib.pyplot as plt
+import numpy as np
+
+def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):
+	plt.clf()
+	fig, ax = plt.subplots(figsize=(12, 8))
+	ax.set_ylabel(ylabel)
+	ax.set_xlabel("Steps")
+	h_list = []
+	for arr, legend, color in zip(arr_list, legend_list, color_list):
+		arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])
+		h = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)
+		arr_err = 1.96 * arr_err
+		ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,
+		                color=color)
+		h_list.append(h)
+	ax.set_title(f"{fig_title}")
+	#ax.legend(handles=h_list)
+	plt.savefig('total_returns.png')
+	#plt.show()
 
 if __name__=='__main__':
 	parser = argparse.ArgumentParser()
@@ -9,7 +29,7 @@ if __name__=='__main__':
 	parser.add_argument('-s', '--seed', type=float, help='Seed for experiment', default=1.0)
 	parser.add_argument('-ns', '--num_steps', type=int, help='Number of steps that the environment should take', default=128)
 	parser.add_argument('-gae', '--gae', type=bool, help='Generalized Advantage Estimation flag', default=True)
-	parser.add_argument('-t', '--total_timesteps', type=int, help='Total number of timesteps that we will take', default=250000)
+	parser.add_argument('-t', '--total_timesteps', type=int, help='Total number of timesteps that we will take', default=500000)
 	parser.add_argument('-al', '--anneal_lr', type=bool, help='How to anneal our learning rate', default=True)
 	parser.add_argument('-gl', '--gae_lambda', type=float, help="the lambda for the general advantage estimation", default=0.95)
 	parser.add_argument('-ue', '--num_update_epochs', type=int, help='The  number of update epochs for the policy', default=4)
@@ -26,10 +46,11 @@ if __name__=='__main__':
 	parser.add_argument('-c', '--continuous', type=bool, help='Is the action space continuous',default=False)
 	parser.add_argument('-lr', '--learning_rate', type=float, help='Learning rate for our agent', default=2e-4)
 	parser.add_argument('-exp', '--exp_name', type=str, help='Experiment name', default='CartPole PPO')
-	parser.add_argument('-nl', '--num_layers', type=int, help='The number of layers in our actor and critic', default=2)
+	parser.add_argument('-nl', '--num_layers', type=int, help='The number of layers in our actor and critic', default=10)
 	parser.add_argument('-do', '--dropout', type=float, help='Dropout in our actor and critic', default=0.0)
 	parser.add_argument('-g', '--gamma', type=float, help='Discount value for rewards', default=0.99)
 	parser.add_argument('-tr', '--track', type=bool, help='Track the performance of the environment', default=True)
+	parser.add_argument('-tri', '--trials', type=int, help='Number of trials to run', default=1)
 	args = parser.parse_args()
 
 	params = {
@@ -61,5 +82,15 @@ if __name__=='__main__':
 		'track':args.track
 	}
 
+	#all_returns = []
+	#all_episode_lengths = []
+	#trials = args.trials
+	#for _ in range(trials):
 	to_run = ppo(params)
-	to_run.train()
\ No newline at end of file
+	total_returns, total_episode_lengths, x_indices = to_run.train()
+	#all_returns.append(total_returns)
+	#all_episode_lengths.append(total_episode_lengths)
+	#print(len(x) for x in all_returns)
+	#print(len(x) for x in all_episode_lengths)
+	#plot_curves(np.array([all_returns]), ['Agent'], ['red'], 'Total Return', args.gym_id)
+	# plot with confidence bands and stuff
\ No newline at end of file
diff --git a/src/wandb/latest-run b/src/wandb/latest-run
index d5194a6..c2964cd 120000
--- a/src/wandb/latest-run
+++ b/src/wandb/latest-run
@@ -1 +1 @@
-run-20231120_002713-4pr5m6kw
\ No newline at end of file
+run-20231123_001914-gfdeytu5
\ No newline at end of file
