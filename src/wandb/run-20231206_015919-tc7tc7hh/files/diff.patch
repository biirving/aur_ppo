diff --git a/plots/episodic lengths_num_layers_2_dropout_0.0_num_envs_1_num_mb_32.png b/plots/episodic lengths_num_layers_2_dropout_0.0_num_envs_1_num_mb_32.png
index c42c87f..1c0a475 100644
Binary files a/plots/episodic lengths_num_layers_2_dropout_0.0_num_envs_1_num_mb_32.png and b/plots/episodic lengths_num_layers_2_dropout_0.0_num_envs_1_num_mb_32.png differ
diff --git a/plots/episodic returns_num_layers_2_dropout_0.0_num_envs_1_num_mb_32.png b/plots/episodic returns_num_layers_2_dropout_0.0_num_envs_1_num_mb_32.png
index aff474a..6d30f92 100644
Binary files a/plots/episodic returns_num_layers_2_dropout_0.0_num_envs_1_num_mb_32.png and b/plots/episodic returns_num_layers_2_dropout_0.0_num_envs_1_num_mb_32.png differ
diff --git a/src/actor_critic_2.pt b/src/actor_critic_2.pt
index 6dd8ef3..f610a3d 100644
Binary files a/src/actor_critic_2.pt and b/src/actor_critic_2.pt differ
diff --git a/src/models/__init__.py b/src/models/__init__.py
index 3254693..51769c8 100644
--- a/src/models/__init__.py
+++ b/src/models/__init__.py
@@ -1 +1,2 @@
-from .actor_critic import actor_critic
\ No newline at end of file
+from .actor_critic import actor_critic
+from .robot_actor_critic import robot_actor_critic
\ No newline at end of file
diff --git a/src/models/actor_critic.py b/src/models/actor_critic.py
index c1f24ec..88b216d 100644
--- a/src/models/actor_critic.py
+++ b/src/models/actor_critic.py
@@ -5,11 +5,6 @@ from torch.distributions import Normal, Categorical
 import numpy as np
 import sys
 
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-# an implementation of the actor-critic 
 class actor_critic(nn.Module):
 	def __init__(self, state_dim:int, action_dim:int, 
 		hidden_dim:int, num_layers:int, 
@@ -25,22 +20,6 @@ class actor_critic(nn.Module):
 		if(continuous):
 			self.actor = continuous_net(hidden_dim, state_dim, action_dim, num_layers, dropout)
 			self.critic = critic(hidden_dim, state_dim, num_layers, dropout)
-			"""
-			self.critic = nn.Sequential(
-				layer_init(nn.Linear(np.array(state_dim).prod(), 64)),
-				nn.Tanh(),
-				layer_init(nn.Linear(64, 64)),
-				nn.Tanh(),
-				layer_init(nn.Linear(64, 1), std=1.0),
-			)
-			self.actor_alt = nn.Sequential(
-				layer_init(nn.Linear(np.array(state_dim).prod(), 64)),
-				nn.Tanh(),
-				layer_init(nn.Linear(64, 64)),
-				nn.Tanh(),
-				layer_init(nn.Linear(64, np.prod(action_dim)), std=0.01),
-			)
-			"""
 			self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(action_dim)))
 		else:
 			self.actor = discrete_net(hidden_dim, state_dim, action_dim, num_layers, dropout)
@@ -49,31 +28,6 @@ class actor_critic(nn.Module):
 	def forward(self):
 		pass
 
-	# set the new action std. We will also set a new action variance
-	def set_action_std(self, new_action_std:float):
-		self.action_std_init = new_action_std
-		self.action_var = torch.full((self.action_dim, ), self.action_std_init ** 2)
-
-	#@profile
-	def act(self, state):
-		if(self.continuous):
-			action_mean = self.actor(state)
-			action_logstd = self.actor_logstd.expand_as(action_mean)
-			action_std = torch.exp(action_logstd)
-			probs = Normal(action_mean, action_std)
-			action = probs.sample()
-			log_prob = probs.log_prob(action).sum(1)
-		else:
-			#hidden = self.network(state)
-			probs = self.actor(state)
-			dist = Categorical(logits=probs)
-			action = dist.sample()
-			log_prob = dist.log_prob(action)
-		
-		value = self.critic(state)
-		# running the policy to produce values for replay buffer. Can detach.
-		return action, log_prob, value
-
 	def value(self, state):
 		return self.critic(state).flatten()
 
diff --git a/src/nets/__init__.py b/src/nets/__init__.py
index 674cfd2..c43fec0 100644
--- a/src/nets/__init__.py
+++ b/src/nets/__init__.py
@@ -1 +1,2 @@
 from .nets import discrete_net, continuous_net, critic
+from .equiv import EquivariantActor, EquivariantCritic
\ No newline at end of file
diff --git a/src/nets/equiv.py b/src/nets/equiv.py
index 6e7e242..2778142 100644
--- a/src/nets/equiv.py
+++ b/src/nets/equiv.py
@@ -1,4 +1,4 @@
-from escnn import nn, gspaces
+from e2cnn import nn, gspaces
 import torch
 
 LOG_SIG_MAX = 2
@@ -6,13 +6,12 @@ LOG_SIG_MIN = -20
 epsilon = 1e-6
 
 class EquivariantEncoder128(torch.nn.Module):
-    def __init__(self, obs_channel=2, n_out=128, initialize=True, N=4):
+    def __init__(self, obs_channel=1, n_out=128, initialize=True, N=4):
         super().__init__()
         self.obs_channel = obs_channel
         self.c4_act = gspaces.Rot2dOnR2(N)
         self.conv = torch.nn.Sequential(
             # 128x128
-
             nn.R2Conv(nn.FieldType(self.c4_act, obs_channel * [self.c4_act.trivial_repr]),
                       nn.FieldType(self.c4_act, n_out//8 * [self.c4_act.regular_repr]),
                       kernel_size=3, padding=1, initialize=initialize),
@@ -56,4 +55,66 @@ class EquivariantEncoder128(torch.nn.Module):
         )
 
     def forward(self, geo):
-        return self.conv(geo)
\ No newline at end of file
+        return self.conv(geo)
+
+
+class EquivariantActor(torch.nn.Module):
+    def __init__(self, obs_shape=(2, 128, 128), action_dim=5, n_hidden=128, initialize=True, N=4):
+        super().__init__()
+        self.obs_channel = obs_shape[0]
+        self.action_dim = action_dim
+        self.c4_act = gspaces.Rot2dOnR2(N)
+        self.n_rho1 = 2 if N==2 else 1
+        self.conv = torch.nn.Sequential(
+            EquivariantEncoder128(self.obs_channel, n_hidden, initialize, N),
+            nn.R2Conv(nn.FieldType(self.c4_act, n_hidden * [self.c4_act.regular_repr]),
+                      # mixed representation including action_dim trivial representations (for the std of all actions),
+                      # (action_dim-2) trivial representations (for the mu of invariant actions),
+                      # and 1 standard representation (for the mu of equivariant actions)
+                      nn.FieldType(self.c4_act, self.n_rho1 * [self.c4_act.irrep(1)] + (action_dim*2-2) * [self.c4_act.trivial_repr]),
+                      kernel_size=1, padding=0, initialize=initialize)
+        )
+
+    def forward(self, obs):
+        batch_size = obs.shape[0]
+        obs_geo = nn.GeometricTensor(obs, nn.FieldType(self.c4_act, self.obs_channel*[self.c4_act.trivial_repr]))
+        conv_out = self.conv(obs_geo).tensor.reshape(batch_size, -1)
+        dxy = conv_out[:, 0:2]
+        inv_act = conv_out[:, 2:self.action_dim]
+        mean = torch.cat((inv_act[:, 0:1], dxy, inv_act[:, 1:]), dim=1)
+        log_std = conv_out[:, self.action_dim:]
+        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)
+        return mean, log_std
+
+
+class EquivariantCritic(torch.nn.Module):
+    def __init__(self, obs_shape=(1, 128, 128), n_hidden=128, initialize=True, N=4):
+        super().__init__()
+        self.obs_channel = obs_shape[0]
+        self.n_hidden = n_hidden
+        # N is which cyclic subgroup we are using
+        # in this case, we are using C_4
+        self.c4_act = gspaces.Rot2dOnR2(N)
+        self.img_conv = EquivariantEncoder128(self.obs_channel, n_hidden, initialize, N)
+        self.n_rho1 = 2 if N==2 else 1
+
+        self.critic = torch.nn.Sequential(
+            # mixed representation including n_hidden regular representations (for the state),
+            # (action_dim-2) trivial representations (for the invariant actions)
+            # and 1 standard representation (for the equivariant actions)
+            nn.R2Conv(nn.FieldType(self.c4_act, n_hidden * [self.c4_act.regular_repr]),
+                      nn.FieldType(self.c4_act, n_hidden * [self.c4_act.regular_repr]),
+                      kernel_size=1, padding=0, initialize=initialize),
+            nn.ReLU(nn.FieldType(self.c4_act, n_hidden * [self.c4_act.regular_repr]), inplace=True),
+            nn.GroupPooling(nn.FieldType(self.c4_act, n_hidden * [self.c4_act.regular_repr])),
+            nn.R2Conv(nn.FieldType(self.c4_act, n_hidden * [self.c4_act.trivial_repr]),
+                      nn.FieldType(self.c4_act, 1 * [self.c4_act.trivial_repr]),
+                      kernel_size=1, padding=0, initialize=initialize),
+        )
+
+    # so in this case, the critic is a Q function ?
+    def forward(self, obs):
+        obs_geo = nn.GeometricTensor(obs, nn.FieldType(self.c4_act, self.obs_channel*[self.c4_act.trivial_repr]))
+        conv_out = self.img_conv(obs_geo)
+        out = self.critic(conv_out)
+        return out 
\ No newline at end of file
diff --git a/src/ppo.py b/src/ppo.py
index 3c9a645..5c7acfd 100644
--- a/src/ppo.py
+++ b/src/ppo.py
@@ -17,12 +17,6 @@ from torch.distributions import Normal, Categorical
 
 device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
 
-def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
-    torch.nn.init.orthogonal_(layer.weight, std)
-    torch.nn.init.constant_(layer.bias, bias_const)
-    return layer
-
-#print(device)
 class torch_buffer():
 	def __init__(self, observation_shape, action_shape, num_steps, num_envs):
 		self.observation_shape = observation_shape
@@ -182,6 +176,7 @@ class ppo():
         "hyperparameters",
         "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{str(self.params_dict[key])}|" for key in self.params_dict])),
     	)
+
 		seed = 1
 		random.seed(seed)
 		np.random.seed(seed)
@@ -269,6 +264,7 @@ class ppo():
 					loss.backward()
 					nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
 					self.optimizer.step()
+
 				if self.target_kl is not None:
 					if approx_kl > self.target_kl:
 						break
@@ -321,4 +317,4 @@ class ppo():
 		plt.xlabel('Timestep')
 		plt.ylabel('Return')
 		plt.legend()
-		plt.savefig('/home/benjamin/Desktop/ml/aur_ppo/plots/' + title + '_num_layers_' + str(self.num_layers) + '_dropout_' + str(self.dropout) + '_num_envs_' + str(self.num_envs) + '_num_mb_' + str(self.num_minibatches) + '.png')
+		plt.savefig('/home/benjamin/Desktop/ml/aur_ppo/plots/' + title + '_num_layers_' + str(self.num_layers) + '_dropout_' + str(self.dropout) + '_num_envs_' + str(self.num_envs) + '_num_mb_' + str(self.num_minibatches) + '.png')
\ No newline at end of file
diff --git a/src/robot_env.py b/src/robot_env.py
index cee780e..df9172b 100644
--- a/src/robot_env.py
+++ b/src/robot_env.py
@@ -1,7 +1,7 @@
-from robot_test import EnvWrapper
+from env_wrapper import EnvWrapper
 import numpy as np
 
-num_processes=5
+num_processes=10
 num_eval_processes=5
 simulator='pybullet'
 env='close_loop_block_pulling'
@@ -10,8 +10,11 @@ env_config={'workspace': np.array([[ 0.25,  0.65],
        [ 0.01,  0.25]]), 'max_steps': 100, 'obs_size': 128, 'fast_mode': True, 'action_sequence': 'pxyzr', 'render': False, 'num_objects': 2, 'random_orientation': True, 'robot': 'kuka', 'workspace_check': 'point', 'object_scale_range': (1, 1), 'hard_reset_freq': 1000, 'physics_mode': 'fast', 'view_type': 'camera_center_xyz', 'obs_type': 'pixel', 'view_scale': 1.5, 'transparent_bin': True}
 planner_config={'random_orientation': True, 'dpos': 0.02, 'drot': 0.19634954084936207}
 envs = EnvWrapper(num_processes, simulator, env, env_config, planner_config)
-print(envs)
-print(envs.reset())
-print(envs.getNextAction())
-eval_envs = EnvWrapper(num_eval_processes, simulator, env, env_config, planner_config)
 
+state, obs = envs.reset()
+print(state)
+act = envs.getNextAction()
+eval_envs = EnvWrapper(num_eval_processes, simulator, env, env_config, planner_config)
+what, what_2, what_3, what_4 = envs.step(act)
+print(what)
+print(what_3)
diff --git a/src/robot_test.py b/src/robot_test.py
deleted file mode 100644
index 61e1741..0000000
--- a/src/robot_test.py
+++ /dev/null
@@ -1,59 +0,0 @@
-import sys
-sys.path.append('..')
-sys.path.append('/work/nlp/b.irving/equi_rl/helping_hands_rl_envs')
-import torch
-from helping_hands_rl_envs import env_factory
-class EnvWrapper:
-    def __init__(self, num_processes, simulator, env, env_config, planner_config):
-        self.envs = env_factory.createEnvs(num_processes, env, env_config, planner_config)
-
-    def reset(self):
-        (states, in_hands, obs) = self.envs.reset()
-        states = torch.tensor(states).float()
-        obs = torch.tensor(obs).float()
-        return states, obs
-
-    def getNextAction(self):
-        return torch.tensor(self.envs.getNextAction()).float()
-
-    def step(self, actions, auto_reset=False):
-        actions = actions.cpu().numpy()
-        (states_, in_hands_, obs_), rewards, dones = self.envs.step(actions, auto_reset)
-        states_ = torch.tensor(states_).float()
-        obs_ = torch.tensor(obs_).float()
-        rewards = torch.tensor(rewards).float()
-        dones = torch.tensor(dones).float()
-        return states_, obs_, rewards, dones
-
-    def stepAsync(self, actions, auto_reset=False):
-        actions = actions.cpu().numpy()
-        self.envs.stepAsync(actions, auto_reset)
-
-    def stepWait(self):
-        (states_, in_hands_, obs_), rewards, dones = self.envs.stepWait()
-        states_ = torch.tensor(states_).float()
-        obs_ = torch.tensor(obs_).float()
-        rewards = torch.tensor(rewards).float()
-        dones = torch.tensor(dones).float()
-        return states_, obs_, rewards, dones
-
-    def getStepLeft(self):
-        return torch.tensor(self.envs.getStepsLeft()).float()
-
-    def reset_envs(self, env_nums):
-        states, in_hands, obs = self.envs.reset_envs(env_nums)
-        states = torch.tensor(states).float()
-        obs = torch.tensor(obs).float()
-        return states, obs
-
-    def close(self):
-        self.envs.close()
-
-    def saveToFile(self, envs_save_path):
-        return self.envs.saveToFile(envs_save_path)
-
-    def getEnvGitHash(self):
-        return self.envs.getEnvGitHash()
-
-    def getEmptyInHand(self):
-        return torch.tensor(self.envs.getEmptyInHand()).float()
\ No newline at end of file
diff --git a/src/run.py b/src/run.py
index 9a1c57c..5187e5c 100644
--- a/src/run.py
+++ b/src/run.py
@@ -26,7 +26,7 @@ def plot_curves(arr_list, legend_list, x_indices, color_list, ylabel, fig_title)
 
 if __name__=='__main__':
 	parser = argparse.ArgumentParser()
-	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='CartPole-v1')
+	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='HalfCheetah-v4')
 	parser.add_argument('-s', '--seed', type=float, help='Seed for experiment', default=1.0)
 	parser.add_argument('-ns', '--num_steps', type=int, help='Number of steps that the environment should take', default=128)
 	parser.add_argument('-gae', '--gae', type=bool, help='Generalized Advantage Estimation flag', default=True)
@@ -45,7 +45,7 @@ if __name__=='__main__':
 	parser.add_argument('-na', '--norm_adv', type=bool, help='Normalize advantage estimates', default=True)
 	parser.add_argument('-p', '--capture_video', type=bool, help='Whether to capture the video or not', default=False)
 	parser.add_argument('-d', '--hidden_dim', type=int, help='Hidden dimension of the neural networks in the actor critic', default=64)
-	parser.add_argument('-c', '--continuous', type=bool, help='Is the action space continuous',default=False)
+	parser.add_argument('-c', '--continuous', type=bool, help='Is the action space continuous',default=True)
 	parser.add_argument('-lr', '--learning_rate', type=float, help='Learning rate for our agent', default=2.5e-4)
 	parser.add_argument('-exp', '--exp_name', type=str, help='Experiment name', default='CartPole PPO')
 	parser.add_argument('-nl', '--num_layers', type=int, help='The number of layers in our actor and critic', default=2)
diff --git a/src/wandb/latest-run b/src/wandb/latest-run
index b419769..09416ac 120000
--- a/src/wandb/latest-run
+++ b/src/wandb/latest-run
@@ -1 +1 @@
-run-20231204_132639-kgrwu2by
\ No newline at end of file
+run-20231206_015919-tc7tc7hh
\ No newline at end of file
