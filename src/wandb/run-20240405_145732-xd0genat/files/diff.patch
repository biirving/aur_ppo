diff --git a/src/models/robot_actor_critic.py b/src/models/robot_actor_critic.py
index 1207755..428eb5c 100644
--- a/src/models/robot_actor_critic.py
+++ b/src/models/robot_actor_critic.py
@@ -126,7 +126,7 @@ class robot_actor_critic(nn.Module):
 		#action_mean = torch.tanh(action_mean)
 		entropy = dist.entropy()		
 		unscaled_actions, actions = self.decodeActions(*[action[:, i] for i in range(self.n_a)])
-		return actions, unscaled_actions, log_prob.sum(1), entropy.sum(1), self.critic(self.network(obs))
+		return actions, unscaled_actions, log_prob.sum(1), entropy.sum(1), self.critic(self.network(cat_obs))
 
 	# pretrain the actor alone
 	def evaluate_pretrain(self, state, obs, action=None):
@@ -145,3 +145,11 @@ class robot_actor_critic(nn.Module):
 		action_mean = torch.tanh(action_mean)
 		unscaled_action, action = self.decodeActions(*[action[:, i] for i in range(self.n_a)])
 		return action.to(torch.float16), unscaled_action.to(torch.float16)
+
+
+	def test_action(self, state, obs):
+		state_tile = state.reshape(state.size(0), 1, 1, 1).repeat(1, 1, obs.shape[2], obs.shape[3])
+		cat_obs = torch.cat([obs, state_tile], dim=1).to(self.device)
+		action_mean = self.actor(self.network(cat_obs))
+		mean = torch.tanh(action_mean)
+		return self.decodeActions(*[mean[:, i] for i in range(self.n_a)])
diff --git a/src/models/sac_core.py b/src/models/sac_core.py
index 6e45bf1..ef8782c 100644
--- a/src/models/sac_core.py
+++ b/src/models/sac_core.py
@@ -10,6 +10,12 @@ from torch.distributions.normal import Normal
 from nets.equiv import EquivariantActor, EquivariantCritic
 from nets.base_cnns import base_actor, base_critic, base_encoder
 
+def weights_init(m):
+    if isinstance(m, nn.Linear):
+        torch.nn.init.xavier_uniform_(m.weight, gain=1)
+        torch.nn.init.constant_(m.bias, 0)
+    elif isinstance(m, nn.Conv2d):
+        nn.init.xavier_normal_(m.weight.data)
 
 def combined_shape(length, shape=None):
     if shape is None:
@@ -32,62 +38,36 @@ LOG_STD_MIN = -20
 
 class SquashedGaussianMLPActor(nn.Module):
 
-    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):
+    def __init__(self, hidden_sizes, activation, 
+        dx=0.02, 
+        dy=0.02, 
+        dz=0.02, 
+        dr=np.pi/8, 
+        n_a=5, 
+        tau=0.001,):
         super().__init__()
+
+        self.p_range = torch.tensor([0, 1])
+        self.dtheta_range = torch.tensor([-dr, dr])
+        self.dx_range = torch.tensor([-dx, dx])
+        self.dy_range = torch.tensor([-dy, dy])
+        self.dz_range = torch.tensor([-dz, dz])	
+        self.n_a = n_a
         #self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)
-        self.net = base_actor()
-        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)
-        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)
-        self.act_limit = act_limit
-
-    # changes to support robot
-    # courtesy of Dian Wang
-    def decodeActions(self, *args):
-        unscaled_p = args[0]
-        unscaled_dx = args[1]
-        unscaled_dy = args[2]
-        unscaled_dz = args[3]
-
-        p = 0.5 * (unscaled_p + 1) * (self.p_range[1] - self.p_range[0]) + self.p_range[0]
-        dx = 0.5 * (unscaled_dx + 1) * (self.dx_range[1] - self.dx_range[0]) + self.dx_range[0]
-        dy = 0.5 * (unscaled_dy + 1) * (self.dy_range[1] - self.dy_range[0]) + self.dy_range[0]
-        dz = 0.5 * (unscaled_dz + 1) * (self.dz_range[1] - self.dz_range[0]) + self.dz_range[0]
-
-        if self.n_a == 5:
-            unscaled_dtheta = args[4]
-            dtheta = 0.5 * (unscaled_dtheta + 1) * (self.dtheta_range[1] - self.dtheta_range[0]) + self.dtheta_range[0]
-            actions = torch.stack([p, dx, dy, dz, dtheta], dim=1)
-            unscaled_actions = torch.stack([unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz, unscaled_dtheta], dim=1)
-        else:
-            actions = torch.stack([p, dx, dy, dz], dim=1)
-            unscaled_actions = torch.stack([unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz], dim=1)
-        return unscaled_actions, actions
-
-	# scaled actions
-    def getActionFromPlan(self, plan):
-        def getUnscaledAction(action, action_range):
-            unscaled_action = 2 * (action - action_range[0]) / (action_range[1] - action_range[0]) - 1
-            return unscaled_action
-        dx = plan[:, 1].clamp(*self.dx_range)
-        p = plan[:, 0].clamp(*self.p_range)
-        dy = plan[:, 2].clamp(*self.dy_range)
-        dz = plan[:, 3].clamp(*self.dz_range)
-        unscaled_p = getUnscaledAction(p, self.p_range)
-        unscaled_dx = getUnscaledAction(dx, self.dx_range)
-        unscaled_dy = getUnscaledAction(dy, self.dy_range)
-        unscaled_dz = getUnscaledAction(dz, self.dz_range)
-        if self.n_a == 5:
-            dtheta = plan[:, 4].clamp(*self.dtheta_range)
-            unscaled_dtheta = getUnscaledAction(dtheta, self.dtheta_range)
-            return self.decodeActions(unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz, unscaled_dtheta)
-        else:
-            return self.decodeActions(unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz)
         
+        #
+        self.net = base_encoder()
+
+        # hidden_sizes
+        self.mu_layer = nn.Linear(hidden_sizes[-1], 5)
+        self.log_std_layer = nn.Linear(hidden_sizes[-1], 5)
+        #self.act_limit = act_limit
+
+    
+
+    def forward(self, obs, deterministic=False, with_logprob=True):
+        net_out = self.net(obs)
 
-    def forward(self, state, obs, deterministic=False, with_logprob=True):
-        state_tile = state.reshape(state.size(0), 1, 1, 1).repeat(1, 1, obs.shape[2], obs.shape[3])
-        cat_obs = torch.cat([obs, state_tile], dim=1).to(self.device)
-        net_out = self.net(cat_obs)
         mu = self.mu_layer(net_out)
         log_std = self.log_std_layer(net_out)
         log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)
@@ -99,6 +79,7 @@ class SquashedGaussianMLPActor(nn.Module):
             # Only used for evaluating policy at test time.
             pi_action = mu
         else:
+
             pi_action = pi_distribution.rsample()
 
         if with_logprob:
@@ -113,43 +94,49 @@ class SquashedGaussianMLPActor(nn.Module):
             logp_pi = None
 
         pi_action = torch.tanh(pi_action)
-        pi_action = self.act_limit * pi_action
+        #pi_action = self.act_limit * pi_action
 
         return pi_action, logp_pi
 
-
-# is this the critic
-# this is a value function
 class MLPQFunction(nn.Module):
-
     def __init__(self, obs_dim, act_dim, hidden_sizes, activation):
         super().__init__()
         self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)
-        
-    # so the q function is processing the action, observation, and state
+
+    # how does this q function actually work?
     def forward(self, obs, act):
         q = self.q(torch.cat([obs, act], dim=-1))
         return torch.squeeze(q, -1) # Critical to ensure q has right shape.
 
 
-# we need to add support for our CNNs
-class MLPActorCritic(nn.Module):
-
-    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),
-                 activation=nn.ReLU):
+class SACCritic(nn.Module):
+    def __init__(self, obs_shape=(2, 128, 128), action_dim=5):
         super().__init__()
+        self.state_conv_1 = base_encoder(obs_shape=(2, 128, 128), out_dim=128)
+        self.critic_fc_1 = torch.nn.Sequential(
+            torch.nn.Linear(128 + action_dim, 128),
+            nn.ReLU(inplace=True),
+            torch.nn.Linear(128, 1)
+        )
+        self.apply(weights_init)
 
-        obs_dim = observation_space.shape[0]
-        act_dim = action_space.shape[0]
-        act_limit = action_space.high[0]
+    def forward(self, obs, act):
+        conv_out = self.state_conv_1(obs)
+        out_1 = self.critic_fc_1(torch.cat((conv_out, act), dim=1))
+        return out_1 
+
+class MLPActorCritic(nn.Module):
+    def __init__(self, n_a=5, hidden_sizes=(1024,1024), activation=nn.ReLU):
+        super().__init__()
 
         # build policy and value functions
-        self.pi = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)
-        self.q1 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)
-        self.q2 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)
+        self.pi = SquashedGaussianMLPActor(hidden_sizes, activation)
+        self.q1 = SACCritic()
+        self.q2 = SACCritic()
 
-    # add the observation in here
-    def act(self, obs, deterministic=False):
+    def act(self, state, obs, deterministic=False):
+        state_tile = state.reshape(state.size(0), 1, 1, 1).repeat(1, 1, obs.shape[2], obs.shape[3])
+        cat_obs = torch.cat([obs, state_tile], dim=1)
         with torch.no_grad():
-            a, _ = self.pi(obs, deterministic, False)
-            return a.numpy()
\ No newline at end of file
+            a, log_pi = self.pi(cat_obs, deterministic, False)
+            return a
\ No newline at end of file
diff --git a/src/robot_ppo.py b/src/robot_ppo.py
index d165a7e..b223555 100644
--- a/src/robot_ppo.py
+++ b/src/robot_ppo.py
@@ -86,6 +86,8 @@ class store_returns():
 		return R, len_episode
 
 class robot_ppo():
+
+	# move the decode actions and get actions from plan into here
 	def __init__(self, params):
 		# accessing through the dictionary is slower
 		self.params_dict = params
@@ -109,7 +111,6 @@ class robot_ppo():
 		self.total_pretrain_steps = self.pretrain_steps * self.num_envs
 		self.pretrain_minibatch_size = int(self.total_pretrain_steps // self.pretrain_batch_size)
 
-
 		num_eval_processes=5
 		simulator='pybullet'
 		env=self.gym_id
@@ -120,7 +121,7 @@ class robot_ppo():
 			'obs_size': 128, 
 			'fast_mode': True, 
 			'action_sequence': 'pxyzr', 
-			'render': True, 
+			'render': self.render, 
 			'num_objects': 2, 
 			'random_orientation': True, 
 			'robot': 'kuka', 
@@ -133,6 +134,7 @@ class robot_ppo():
 			'view_scale': 1.5, 
 			'transparent_bin': True}
 		planner_config={'random_orientation': True, 'dpos': 0.02, 'drot': 0.19634954084936207}
+
 		self.envs = EnvWrapper(self.num_envs, simulator, env, env_config, planner_config)
 		self.eval_envs = EnvWrapper(num_eval_processes, simulator, env, env_config, planner_config)
 		self.plot_index = 0 
@@ -170,10 +172,12 @@ class robot_ppo():
 		# the true action
 		# the gradients of the expert should not be stored?
 		with torch.no_grad():	
-			_, true_action, _, _, _ = self.expert.evaluate(next_state.to(device), next_obs.to(device))
+			true_action = self.envs.getNextAction()
+			true_action, scaled = self.policy.getActionFromPlan(true_action)
+			#_, true_action, _, _, _ = self.policy.evaluate(next_state.to(device), next_obs.to(device))
 		self.buffer.true_actions[step] = true_action 
-		next_states, next_obs, reward, done = self.envs.step(actions, auto_reset=True)
-		print('next state', next_states.shape)
+		self.envs.stepAsync(actions, auto_reset=True)
+		next_states, next_obs, reward, done = self.envs.stepWait()
 		for i, rew in enumerate(reward):
 			self.episodic_returns.add_value(i, rew)
 		self.buffer.rewards[step] = reward.view(-1)
@@ -192,8 +196,8 @@ class robot_ppo():
 	def expert_rollout(self, step, state, obs):
 		with torch.no_grad():
 			true_action = self.envs.getNextAction()
-			unscaled, scaled = self.expert.getActionFromPlan(true_action)
-			scaled_agent, unscaled_agent, logprob, _, value = self.expert.evaluate(state.to(device), obs.to(device))
+			unscaled, scaled = self.policy.getActionFromPlan(true_action)
+			scaled_agent, unscaled_agent, logprob, _, value = self.policy.evaluate(state.to(device), obs.to(device))
 			# to store for mse loss
 			# why use the unscaled action?
 			self.pretrain_buffer.actions[step] = unscaled_agent 
@@ -284,8 +288,24 @@ class robot_ppo():
 				self.pretrain_optimizer.zero_grad()
 				loss = self.expert_weight * expert_loss
 				loss.backward()
-				nn.utils.clip_grad_norm_(self.expert.actor.parameters(), self.max_grad_norm)
-				self.pretrain_optimizer.step()
+				nn.utils.clip_grad_norm_(self.policy.actor.parameters(), self.max_grad_norm)
+				self.optimizer.step()
+
+	def test_env(self, writer):
+		with torch.no_grad():
+			num_eval_steps=1000
+			state, obs = self.envs.reset()
+			for step in range(num_eval_steps):
+				# do we have auto_reset = true?
+				#u_a, a = self.expert.test_action(state.to(device), obs.to(device))
+				scaled_agent, unscaled_agent, logprob, _, value = self.expert.evaluate(state.to(device), obs.to(device))
+				state, obs, reward, done = self.envs.step(scaled_agent, auto_reset=True)
+				for i, d in enumerate(done):
+					if d:
+						discounted_return, episode_length = self.episodic_returns.calc_discounted_return(i)
+						print('Episode length', episode_length)
+						writer.add_scalar("charts/test_discounted_episodic_return", discounted_return, step)
+						writer.add_scalar("charts/test_episodic_length", episode_length, step)
 
 	def update(self, buffer, update_epochs, batch_size, minibatch_size, policy_losses):
 		assert minibatch_size != 0
@@ -350,8 +370,8 @@ class robot_ppo():
 
 				# change this to be the KL divergence
 				# they use a lagrangian equation to optimize
-				expert_loss = nn.functional.kl_div(b_actions[mb_inds], b_true_actions[mb_inds])
-				#expert_loss = nn.functional.mse_loss(b_actions[mb_inds], b_true_actions[mb_inds])
+				#expert_loss = nn.functional.kl_div(b_actions[mb_inds], b_true_actions[mb_inds])
+				expert_loss = nn.functional.mse_loss(b_actions[mb_inds], b_true_actions[mb_inds])
 				loss = policy_loss - self.entropy_coeff * entropy_loss + value_loss + self.expert_weight * expert_loss
 
 				loss.backward()
@@ -398,6 +418,8 @@ class robot_ppo():
 			self.pretrain_buffer.load_to_cpu()
 			print('Pretraining complete')
 
+		self.test_env(writer)
+
 		global_step = 0
 		start_time = time.time()
 		next_state, next_obs = self.envs.reset()
diff --git a/src/robot_run.py b/src/robot_run.py
index c257df8..7e6dc45 100644
--- a/src/robot_run.py
+++ b/src/robot_run.py
@@ -8,6 +8,16 @@ import os, sys, argparse, time
 import matplotlib.pyplot as plt
 import numpy as np
 
+def str2bool(v):
+    if isinstance(v, bool):
+       return v
+    if v.lower() in ('yes', 'true', 't', 'y', '1'):
+        return True
+    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
+        return False
+    else:
+        raise argparse.ArgumentTypeError('Boolean value expected.')
+
 def plot_curves(arr_list, legend_list, x_indices, color_list, ylabel, fig_title):
 	plt.clf()
 	fig, ax = plt.subplots(figsize=(12, 8))
@@ -29,16 +39,16 @@ def plot_curves(arr_list, legend_list, x_indices, color_list, ylabel, fig_title)
 
 if __name__=='__main__':
 	parser = argparse.ArgumentParser()
-	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='close_loop_block_in_bowl')
+	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='close_loop_block_reaching')
 	parser.add_argument('-s', '--seed', type=float, help='Seed for experiment', default=1.0)
 	parser.add_argument('-gae', '--gae', type=bool, help='Generalized Advantage Estimation flag', default=True)
 
 	# ----- TIME -----
-	parser.add_argument('-ns', '--num_steps', type=int, help='Number of steps that the environment should take', default=128)
+	parser.add_argument('-ns', '--num_steps', type=int, help='Number of steps that the environment should take', default=1024)
 	parser.add_argument('-t', '--total_timesteps', type=int, help='Total number of timesteps that we will take', default=50000)
 	parser.add_argument('-ue', '--num_update_epochs', type=int, help='The  number of update epochs for the policy', default=10)
 	parser.add_argument('-pte', '--pretrain_episodes', type=int, help='Number of pretraining episodes', default=100)
-	parser.add_argument('-pts', '--pretrain_steps', type=int, help='Number of pretraining steps', default=5000)
+	parser.add_argument('-pts', '--pretrain_steps', type=int, help='Number of pretraining steps', default=1000)
 	parser.add_argument('-ptb', '--pretrain_batch_size', type=int, help='The size of our pretrain batch', default=8)
 
 	# ----- Variables to change ------
@@ -59,7 +69,7 @@ if __name__=='__main__':
 	parser.add_argument('-na', '--norm_adv', type=bool, help='Normalize advantage estimates', default=True)
 	parser.add_argument('-p', '--capture_video', type=bool, help='Whether to capture the video or not', default=False)
 	parser.add_argument('-d', '--hidden_dim', type=int, help='Hidden dimension of the neural networks in the actor critic', default=64)
-	parser.add_argument('-c', '--continuous', type=lambda x: bool(strtobool(x)), default=True, nargs='?', const=True)
+	parser.add_argument('-c', '--continuous', type=str2bool, default=True, nargs='?', const=True)
 	parser.add_argument('-exp', '--exp_name', type=str, help='Experiment name', default='close_loop_block_pulling')
 	parser.add_argument('-nl', '--num_layers', type=int, help='The number of layers in our actor and critic', default=2)
 	parser.add_argument('-do', '--dropout', type=float, help='Dropout in our actor and critic', default=0.0)
@@ -69,6 +79,8 @@ if __name__=='__main__':
 	parser.add_argument('-eq', '--equivariant', type=bool, help='Run the robot with equivariant networks, or not', default=False)
 	parser.add_argument('-anexp', '--anneal_exp', type=bool, help='Do we want to anneal the expert weight?', default=False)
 	parser.add_argument('-sfp', '--save_file_path', type=str, help='Where to save model to', default=None)
+	parser.add_argument('-render', '--render', type=str2bool, help='Whether or not to render the environment', default=False, nargs='?', const=False)
+	parser.add_argument('-dpr', '--do_pretraining', type=str2bool, help='Whether or not to render the environment', default=True, nargs='?', const=True)
 	args = parser.parse_args()
 
 	params = {
@@ -105,7 +117,9 @@ if __name__=='__main__':
 		'pretrain_batch_size':args.pretrain_batch_size,
 		'expert_weight':args.expert_weight,
 		'anneal_exp':args.anneal_exp,
-		'save_file_path':args.save_file_path
+		'save_file_path':args.save_file_path,
+        'render':args.render,
+        'do_pretraining':args.do_pretraining
 	}
 
 	#all_returns = []
diff --git a/src/sac.py b/src/sac.py
index bed905d..c3a5f70 100644
--- a/src/sac.py
+++ b/src/sac.py
@@ -1,21 +1,41 @@
-# I need a sanity check
-
+# sanity check
 # open ai SAC implementation, with a few changes
 
-
 from copy import deepcopy
 import itertools
 import numpy as np
 import torch
 from torch.optim import Adam
-import gym
 import time
-import aur_ppo.src.models.sac_core as core
+import sys
+
+sys.path.append('/work/nlp/b.irving/aur_ppo/src')
+import models.sac_core as core
+from utils.str2bool import str2bool
+from env_wrapper_2 import EnvWrapper
+
+from torch.utils.tensorboard import SummaryWriter
+from tqdm import tqdm
+import argparse
+
+# need to change in order to support parallel processing
+# as of now, this code supports one environment
+
+# add support for the different buffers
+
+
+# TODO: Add parallelization
+# why doesn't PPO work
 
 
+# This is a bad design for the replay buffer, because it can not be arbitrarily extended
+# based on whatever
+# should grow dynamically
 class ReplayBuffer:
     """
     A simple FIFO experience replay buffer for SAC agents.
+
+    What are other experience replay buffers that could be used instead?
     """
 
     def __init__(self, obs_dim, act_dim, size):
@@ -29,7 +49,7 @@ class ReplayBuffer:
         self.ptr, self.size, self.max_size = 0, 0, size
 
     # how are the states of the grippers going to be stored?
-    def store(self, obs, state, act, rew, next_state, next_obs, done):
+    def store(self, obs, state, act, rew, next_obs, next_state, done):
         self.obs_buf[self.ptr] = obs
         self.obs2_buf[self.ptr] = next_obs
         self.states_buf_1[self.ptr] = state
@@ -41,7 +61,7 @@ class ReplayBuffer:
         self.size = min(self.size+1, self.max_size)
 
     def sample_batch(self, batch_size=32):
-        idxs = np.random.randint(0, self.size, size=batch_size)
+        idxs = np.random.randint(0, self.ptr, size=batch_size)
         batch = dict(obs=self.obs_buf[idxs],
                      obs2=self.obs2_buf[idxs],
                      states=self.states_buf_1[idxs],
@@ -50,13 +70,78 @@ class ReplayBuffer:
                      rew=self.rew_buf[idxs],
                      done=self.done_buf[idxs])
         return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}
-
-
-
-def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0, 
-        steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99, 
-        polyak=0.995, lr=1e-3, alpha=0.2, batch_size=100, start_steps=10000, 
-        update_after=1000, update_every=50, num_test_episodes=10, max_ep_len=1000, save_freq=1):
+    
+    
+
+def normalizeTransition(obs, next_obs):
+    obs = np.clip(obs, 0, 0.32)
+    obs = obs/0.4*255
+    obs = obs.astype(np.uint8)
+
+    next_obs = np.clip(next_obs, 0, 0.32)
+    next_obs = next_obs/0.4*255
+    next_obs = next_obs.astype(np.uint8)
+    return obs, next_obs
+
+# moving decode and get action from plan functions
+dx=0.02 
+dy=0.02 
+dz=0.02
+dr=np.pi/8
+n_a=5
+p_range = torch.tensor([0, 1])
+dtheta_range = torch.tensor([-dr, dr])
+dx_range = torch.tensor([-dx, dx])
+dy_range = torch.tensor([-dy, dy])
+dz_range = torch.tensor([-dz, dz])	
+n_a = n_a
+
+def decodeActions(*args):
+        unscaled_p = args[0]
+        unscaled_dx = args[1]
+        unscaled_dy = args[2]
+        unscaled_dz = args[3]
+
+        p = 0.5 * (unscaled_p + 1) * (p_range[1] - p_range[0]) + p_range[0]
+        dx = 0.5 * (unscaled_dx + 1) * (dx_range[1] - dx_range[0]) + dx_range[0]
+        dy = 0.5 * (unscaled_dy + 1) * (dy_range[1] - dy_range[0]) + dy_range[0]
+        dz = 0.5 * (unscaled_dz + 1) * (dz_range[1] - dz_range[0]) + dz_range[0]
+
+        if n_a == 5:
+            unscaled_dtheta = args[4]
+            dtheta = 0.5 * (unscaled_dtheta + 1) * (dtheta_range[1] - dtheta_range[0]) + dtheta_range[0]
+            actions = torch.stack([p, dx, dy, dz, dtheta], dim=1)
+            unscaled_actions = torch.stack([unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz, unscaled_dtheta], dim=1)
+        else:
+            actions = torch.stack([p, dx, dy, dz], dim=1)
+            unscaled_actions = torch.stack([unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz], dim=1)
+        return unscaled_actions, actions
+
+# scaled actions
+def getActionFromPlan(plan):
+    def getUnscaledAction(action, action_range):
+        unscaled_action = 2 * (action - action_range[0]) / (action_range[1] - action_range[0]) - 1
+        return unscaled_action
+    dx = plan[:, 1].clamp(*dx_range)
+    p = plan[:, 0].clamp(*p_range)
+    dy = plan[:, 2].clamp(*dy_range)
+    dz = plan[:, 3].clamp(*dz_range)
+    unscaled_p = getUnscaledAction(p, p_range)
+    unscaled_dx = getUnscaledAction(dx, dx_range)
+    unscaled_dy = getUnscaledAction(dy, dy_range)
+    unscaled_dz = getUnscaledAction(dz, dz_range)
+    if n_a == 5:
+        dtheta = plan[:, 4].clamp(*dtheta_range)
+        unscaled_dtheta = getUnscaledAction(dtheta, dtheta_range)
+        return decodeActions(unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz, unscaled_dtheta)
+    else:
+        return decodeActions(unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz)
+
+def sac(envs, test_envs, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0, 
+        num_processes=1, steps_per_epoch=1000, epochs=4, replay_size=int(1e5), gamma=0.99, 
+        polyak=0.995, lr=1e-3, alpha=0.2, batch_size=64, start_steps=10000, 
+        update_after=1000, update_every=50, pretrain_episodes=1000, num_test_episodes=10, 
+        max_ep_len=100,track=False, save_freq=1, gym_id=None, device=torch.device('cpu')):
     """
     Soft Actor-Critic (SAC)
 
@@ -155,19 +240,34 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
     """
 
 
+    if track:
+        import wandb
+        wandb.init(project='ppo',entity='Aurelian',sync_tensorboard=True,config=None,name=gym_id + '_' + str(lr)) #+ '_' 
+       # str(self.value_coeff) + '_' + str(self.entropy_coeff) + '_' + str(self.clip_coeff) + '_' + str(self.num_minibatches),monitor_gym=True,save_code=True)
+    writer = SummaryWriter(f"runs/{gym_id}")
+    writer.add_text("parameters/what", "what")
+    #writer.add_text(
+    #"hyperparameters",
+    #"|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{str(self.params_dict[key])}|" for key in self.params_dict])),
+    #)
+
+
     torch.manual_seed(seed)
     np.random.seed(seed)
 
-    env, test_env = env_fn(), env_fn()
-    obs_dim = env.observation_space.shape
-    act_dim = env.action_space.shape[0]
+    # will this make two copies
+    env, test_env = envs, test_envs
+    obs_dim = (1, 128, 128)
+    #act_dim = env.action_space.shape[0]
+    act_dim=5
 
     # Action limit for clamping: critically, assumes all dimensions share the same bound!
-    act_limit = env.action_space.high[0]
+    #act_limit = env.action_space.high[0]
 
     # Create actor-critic module and target networks
-    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)
-    ac_targ = deepcopy(ac)
+
+    ac = core.MLPActorCritic().to(device)
+    ac_targ = deepcopy(ac.to(device))
 
     # Freeze target networks with respect to optimizers (only update via polyak averaging)
     for p in ac_targ.parameters():
@@ -184,20 +284,36 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
 
     # Set up function for computing SAC Q-losses
     def compute_loss_q(data):
-        o, s, a, r, o2, s2, d = data['obs'], data['states'], data['act'], data['rew'], data['obs2'], data['states2'], data['done']
-
-        q1 = ac.q1(o,a)
-        q2 = ac.q2(o,a)
+        o, s, a, r, o2, s2, d = (data['obs'].float(), 
+        data['states'].float(), 
+        data['act'].float().to(device), 
+        # should the reward be converted as well
+        data['rew'].float().to(device), 
+        data['obs2'].float(), 
+        data['states2'].float(),
+        data['done'].float().to(device))
+
+        # these will be numpy arrays
+        # so we can change them to tensors?
+        # concatenate for loss
+        s_tile = s.reshape(s.size(0), 1, 1, 1).repeat(1, 1, o.shape[2], o.shape[3])
+        cat_o = torch.cat([o, s_tile], dim=1).to(device)
+
+        s_tile_2 = s2.reshape(s2.size(0), 1, 1, 1).repeat(1, 1, o2.shape[2], o2.shape[3])
+        cat_o_2 = torch.cat([o2, s_tile_2], dim=1).to(device)
+        q1 = ac.q1(cat_o,a)
+        q2 = ac.q2(cat_o,a)
 
         # Bellman backup for Q functions
         with torch.no_grad():
             # Target actions come from *current* policy
-            a2, logp_a2 = ac.pi(o2)
+            a_coded, logp_a2 = ac.pi(cat_o_2)
+            u_a2, a2 = decodeActions(*[a_coded[:, i] for i in range(n_a)])
 
             # Target Q-values
-            q1_pi_targ = ac_targ.q1(o2, a2)
-            q2_pi_targ = ac_targ.q2(o2, a2)
-            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)
+            q1_pi_targ = ac_targ.q1(cat_o_2, u_a2)
+            q2_pi_targ = ac_targ.q2(cat_o_2, u_a2)
+            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ).to(device)
             backup = r + gamma * (1 - d) * (q_pi_targ - alpha * logp_a2)
 
         # MSE loss against Bellman backup
@@ -206,24 +322,37 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
         loss_q = loss_q1 + loss_q2
 
         # Useful info for logging
-        q_info = dict(Q1Vals=q1.detach().numpy(),
-                      Q2Vals=q2.detach().numpy())
+        q_info = dict(Q1Vals=q1.cpu().detach().numpy(),
+                      Q2Vals=q2.cpu().detach().numpy())
 
         return loss_q, q_info
 
     # Set up function for computing SAC pi loss
     def compute_loss_pi(data):
-        o = data['obs']
-        pi, logp_pi = ac.pi(o)
-        q1_pi = ac.q1(o, pi)
-        q2_pi = ac.q2(o, pi)
+
+        o = data['obs'].float()
+        s = data['states'].float()
+
+        # that means we have to compute the cat obs manually
+        # convert to tensors before processing
+
+        s_tile = s.reshape(s.size(0), 1, 1, 1).repeat(1, 1, o.shape[2], o.shape[3])
+        cat_o = torch.cat([o, s_tile], dim=1).to(device)
+
+        # need to change this
+        a_coded, logp_pi = ac.pi(cat_o)
+        u_pi, pi = decodeActions(*[a_coded[:, i] for i in range(n_a)])
+
+        q1_pi = ac.q1(cat_o, pi)
+        q2_pi = ac.q2(cat_o, pi)
+
         q_pi = torch.min(q1_pi, q2_pi)
 
         # Entropy-regularized policy loss
         loss_pi = (alpha * logp_pi - q_pi).mean()
 
         # Useful info for logging
-        pi_info = dict(LogPi=logp_pi.detach().numpy())
+        pi_info = dict(LogPi=logp_pi.cpu().detach().numpy())
 
         return loss_pi, pi_info
 
@@ -267,37 +396,111 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
                 p_targ.data.mul_(polyak)
                 p_targ.data.add_((1 - polyak) * p.data)
 
-    def get_action(o, deterministic=False):
-        return ac.act(torch.as_tensor(o, dtype=torch.float32), 
+    def get_action(s, o, deterministic=False):
+        return ac.act(torch.from_numpy(s).float().to(device), torch.from_numpy(o).float().to(device),
                       deterministic)
 
     def test_agent():
-        for j in range(num_test_episodes):
-            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0
+        ep_ret, ep_len = 0, 0
+        for j in tqdm(range(num_test_episodes)):
+            #o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0
+            s, o = test_env.reset()
+            d, ep_ret, ep_len = False, 0, 0
             while not(d or (ep_len == max_ep_len)):
-                # Take deterministic actions at test time 
-                o, r, d, _ = test_env.step(get_action(o, True))
+                a_coded  = get_action(s, o, False)
+                _, a = decodeActions(*[a_coded[:, i] for i in range(n_a)])
+                s, o, r, d = test_env.step(a)
                 ep_ret += r
                 ep_len += 1
+        return ep_ret, ep_len
+
+    # following Dians code really closely for the pretrainin
+    counter = 0
+    if pretrain_episodes > 0:
+        planner_envs = env
+
+        planner_num_process = num_processes
+        j = 0
+        states, obs = planner_envs.reset()
+        s = 0
+
+        #if not no_bar:
+        planner_bar = tqdm(total=pretrain_episodes)
+        local_transitions = []
+        while j < pretrain_episodes:
+            plan_actions = planner_envs.getNextAction()
+            # it doesn't matter what agent we use
+            planner_actions_star_idx, planner_actions_star = getActionFromPlan(plan_actions)
+            states_, obs_, rewards, dones = planner_envs.step(planner_actions_star, auto_reset=True)
+            #steps_lefts = planner_envs.getStepLeft()
+
+            # then iterate through each environment
+            for i in range(planner_num_process):
+                obs[i], obs_[i] = normalizeTransition(obs[i], obs_[i])
+                transition = (states[i], obs[i], planner_actions_star_idx[i],
+                                              rewards[i], states_[i], obs_[i], dones[i])
+                                              #steps_lefts[i].numpy(), np.array(1))
+                local_transitions.append(transition)
+            states = deepcopy(states_)
+            obs = deepcopy(obs_)
+            if dones.sum() and rewards.sum():
+                for t in local_transitions:
+                    # store in the replay buffer
+                    # TODO: ADD TRANSITION TO REPLAY BUFFER
+                    replay_buffer.store(t[1], t[0], t[2], t[3], t[5], t[4], t[6])
+
+                local_transitions = []
+                j += dones.sum().item()
+                s += rewards.sum().item()
+                #if not no_bar:
+                planner_bar.set_description('{:.3f}/{}, AVG: {:.3f}'.format(s, j, float(s) / j if j != 0 else 0))
+                planner_bar.update(dones.sum().item())
+
+            elif dones.sum():
+                local_transitions = []
+
+            counter += 1
+            # adding data augmentation and weighted buffer?
+            #if expert_aug_n > 0:
+            #    augmentBuffer(replay_buffer, buffer_aug_type, expert_aug_n)
+    print('Total size ', counter)
+    # separate method for the buffer
+    # how do we determine the number of pretraining steps
+    # versus the number of planning episodes?
+    # how to define pretraining batchsize
+    # however many local transitions we want to process I guess?
+    pretrain_step = counter // batch_size
+    if pretrain_step > 0:
+        pbar = tqdm(total=pretrain_step)
+        for i in range(pretrain_step):
+            batch = replay_buffer.sample_batch(batch_size)
+            update(data=batch)
+        
 
     # Prepare for interaction with environment
     total_steps = steps_per_epoch * epochs
     start_time = time.time()
-    s, o, ep_ret, ep_len = env.reset(), 0, 0
+
+    s, o  = env.reset()
+    ep_ret, ep_len = 0, 0
+    last_ret, last_len = 0, 0
 
     # Main loop: collect experience in env and update/log each epoch
-    for t in range(total_steps):
+    replay_len=0
+    for t in tqdm(range(total_steps)):
         
+
+        # here, we can make some changes to the training loop
         # Until start_steps have elapsed, randomly sample actions
         # from a uniform distribution for better exploration. Afterwards, 
         # use the learned policy. 
-        if t > start_steps:
-            a = get_action(o)
-        else:
-            a = env.action_space.sample()
+        a_coded = get_action(s, o)
+        u_a, a = decodeActions(*[a_coded[:, i] for i in range(n_a)])
+        u_a, a = u_a.cpu().numpy(), a.cpu().numpy()
+
 
         # Step the env
-        s2, o2, r, d, _ = env.step(a)
+        s2, o2, r, d  = env.step(a)
         ep_ret += r
         ep_len += 1
 
@@ -307,21 +510,27 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
         d = False if ep_len==max_ep_len else d
 
         # Store experience to replay buffer
-        replay_buffer.store(o, s, a, r, o2, d)
+        o, o2 = normalizeTransition(o, o2)
+        replay_buffer.store(o, s, u_a, r, o2, s2, d)
+        replay_len+=1
 
         # Super critical, easy to overlook step: make sure to update 
         # most recent observation!
         o = o2
+        # update the state as well?
+        s = s2
 
         # End of trajectory handling
         if d or (ep_len == max_ep_len):
-            o, ep_ret, ep_len = env.reset(), 0, 0
+            s, o = env.reset()
+            last_ret = ep_ret
+            last_len = ep_len
+            ep_ret, ep_len = 0,0
 
         # Update handling
-        if t >= update_after and t % update_every == 0:
-            for j in range(update_every):
-                batch = replay_buffer.sample_batch(batch_size)
-                update(data=batch)
+        if replay_len >= 100:
+            batch = replay_buffer.sample_batch(batch_size)
+            update(data=batch)
 
         # End of epoch handling
         if (t+1) % steps_per_epoch == 0:
@@ -331,24 +540,58 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
             #if (epoch % save_freq == 0) or (epoch == epochs):
 
             # Test the performance of the deterministic version of the agent.
-            test_agent()
+
+            #last_ret, last_len = test_agent()
+            #print('test return', last_ret)
+            #print('test length', last_len)
 
             # Log info about epoch
+            # here, we add the episode reward and the global timestep
+            #writer.add_scalar("charts/discounted_episodic_return", last_ret, t)
+            #writer.add_scalar("charts/episodic_length", last_len, t)
 
 if __name__ == '__main__':
-    import argparse
     parser = argparse.ArgumentParser()
     #parser.add_argument('--env', type=str, default='HalfCheetah-v2')
-    parser.add_argument('--hid', type=int, default=256)
+    parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='close_loop_block_reaching')
+    parser.add_argument('--hid', type=int, default=1024)
     parser.add_argument('--l', type=int, default=2)
     parser.add_argument('--gamma', type=float, default=0.99)
     parser.add_argument('--seed', '-s', type=int, default=0)
     parser.add_argument('--epochs', type=int, default=50)
     parser.add_argument('--exp_name', type=str, default='sac')
+    parser.add_argument('-tr', '--track', type=str2bool, help='Track the performance of the environment', nargs='?', const=False, default=False)
+    parser.add_argument('-ne', '--num_envs', type=int, default=1)
+    parser.add_argument('-re', '--render', type=str2bool, nargs='?', const=False, default=False)
     args = parser.parse_args()
 
+    simulator='pybullet'
+    env_config={'workspace': np.array([[ 0.25,  0.65],
+            [-0.2 ,  0.2 ],
+            [ 0.01,  0.25]]), 'max_steps': 100, 
+            'obs_size': 128, 
+            'fast_mode': True, 
+            'action_sequence': 'pxyzr', 
+            'render': args.render, 
+            'num_objects': 2, 
+            'random_orientation': True, 
+            'robot': 'kuka', 
+            'workspace_check': 'point', 
+            'object_scale_range': (1, 1), 
+            'hard_reset_freq': 1000, 
+            'physics_mode': 'fast', 
+            'view_type': 'camera_center_xyz', 
+            'obs_type': 'pixel', 
+            'view_scale': 1.5, 
+            'transparent_bin': True}
+    planner_config={'random_orientation': True, 'dpos': 0.02, 'drot': 0.19634954084936207}
+    envs = EnvWrapper(args.num_envs, simulator, args.gym_id, env_config, planner_config)
+    test_envs = EnvWrapper(args.num_envs, simulator, args.gym_id, env_config, planner_config)
     torch.set_num_threads(torch.get_num_threads())
 
-    sac(lambda : gym.make(args.env), actor_critic=core.MLPActorCritic,
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+
+    sac(envs, test_envs, actor_critic=core.MLPActorCritic,
         ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), 
-        gamma=args.gamma, seed=args.seed, epochs=args.epochs)
\ No newline at end of file
+        gamma=args.gamma, seed=args.seed, num_processes=args.num_envs, epochs=args.epochs,
+        track=args.track, gym_id=args.gym_id, device=device)
\ No newline at end of file
