diff --git a/src/models/robot_actor_critic.py b/src/models/robot_actor_critic.py
index deebd93..377b8da 100644
--- a/src/models/robot_actor_critic.py
+++ b/src/models/robot_actor_critic.py
@@ -77,6 +77,7 @@ class robot_actor_critic(nn.Module):
 		# support batching
 		state_tile = state.reshape(state.size(0), 1, 1, 1).repeat(1, 1, obs.shape[2], obs.shape[3])
 		cat_obs = torch.cat([obs, state_tile], dim=1).to(self.device)
+		# maybe the log std should still be a parameter
 		action_mean, action_logstd = self.actor(cat_obs)
 		action_std = torch.exp(action_logstd)
 		dist = Normal(action_mean, action_std)
diff --git a/src/nets/equiv.py b/src/nets/equiv.py
index 2778142..3637e39 100644
--- a/src/nets/equiv.py
+++ b/src/nets/equiv.py
@@ -117,4 +117,7 @@ class EquivariantCritic(torch.nn.Module):
         obs_geo = nn.GeometricTensor(obs, nn.FieldType(self.c4_act, self.obs_channel*[self.c4_act.trivial_repr]))
         conv_out = self.img_conv(obs_geo)
         out = self.critic(conv_out)
-        return out 
\ No newline at end of file
+        return out 
+
+
+    
\ No newline at end of file
diff --git a/src/robot_ppo.py b/src/robot_ppo.py
index 9c9f9b1..9df2484 100644
--- a/src/robot_ppo.py
+++ b/src/robot_ppo.py
@@ -151,8 +151,8 @@ class robot_ppo():
 			if d:
 				discounted_return, episode_length = self.episodic_returns.calc_discounted_return(i)
 				# then we get the discounted episodic return for env 'i'
-				writer.add_scalar("charts/discounted_episodic_return", discounted_return, self.plot_index)
-				writer.add_scalar("charts/episodic_length", episode_length, self.plot_index)
+				writer.add_scalar("charts/discounted_episodic_return", discounted_return, global_step)
+				writer.add_scalar("charts/episodic_length", episode_length, global_step)
 				self.plot_index += 1
 				break
 				#how do we deal with multiple episodes ending on the same global step?
@@ -203,18 +203,15 @@ class robot_ppo():
 		return returns, advantages
 
 	def normalizeTransition(self, obs):
-		#obs = torch.clip(obs, 0, 0.32)
-		#obs = obs/0.4*255
+		obs = torch.clip(obs, 0, 0.32)
+		obs = obs/0.4*255
 		#obs = obs.to(torch.uint8)
 		return obs
 
 	#simple imitation learning pretraining of our agent
-	# is this completely wrong
 	def pretrain(self):
 		loss_fct = torch.nn.MSELoss()
 		state, obs = self.envs.reset()
-		# we want to pretrain our policy... use simple MSE loss
-		# using the distributed environment?
 		index = 0
 		p = 0
 		while p < self.pretrain_episodes:
diff --git a/src/robot_run.py b/src/robot_run.py
index b483626..e359635 100644
--- a/src/robot_run.py
+++ b/src/robot_run.py
@@ -54,7 +54,7 @@ if __name__=='__main__':
 	parser.add_argument('-tr', '--track', type=bool, help='Track the performance of the environment', default=False)
 	parser.add_argument('-tri', '--trials', type=int, help='Number of trials to run', default=1)
 	parser.add_argument('-eq', '--equivariant', type=bool, help='Run the robot with equivariant networks, or not', default=True)
-	parser.add_argument('-pte', '--pretrain_episodes', type=int, help='Number of pretraining episodes', default=100)
+	parser.add_argument('-pte', '--pretrain_episodes', type=int, help='Number of pretraining episodes', default=1)
 	parser.add_argument('-ptb', '--pretrain_batch_size', type=int, help='The size of our pretrain batch', default=8)
 	args = parser.parse_args()
 
diff --git a/src/run.py b/src/run.py
index 5187e5c..de3281c 100644
--- a/src/run.py
+++ b/src/run.py
@@ -1,9 +1,11 @@
 #!/usr/bin/env python
 import torch
 from ppo import ppo
+from robot_ppo import robot_ppo
 import os, sys, argparse, time
 import matplotlib.pyplot as plt
 import numpy as np
+from distutils.util import strtobool
 
 def plot_curves(arr_list, legend_list, x_indices, color_list, ylabel, fig_title):
 	plt.clf()
@@ -27,11 +29,12 @@ def plot_curves(arr_list, legend_list, x_indices, color_list, ylabel, fig_title)
 if __name__=='__main__':
 	parser = argparse.ArgumentParser()
 	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='HalfCheetah-v4')
+	parser.add_argument('-rb', '--robot', type=lambda x: bool(strtobool(x)), default=False, nargs='?', const=False)
 	parser.add_argument('-s', '--seed', type=float, help='Seed for experiment', default=1.0)
 	parser.add_argument('-ns', '--num_steps', type=int, help='Number of steps that the environment should take', default=128)
-	parser.add_argument('-gae', '--gae', type=bool, help='Generalized Advantage Estimation flag', default=True)
+	parser.add_argument('-gae', '--gae', type=lambda x: bool(strtobool(x)), help='Generalized Advantage Estimation flag', default=True, nargs='?', const=True)
 	parser.add_argument('-t', '--total_timesteps', type=int, help='Total number of timesteps that we will take', default=500000)
-	parser.add_argument('-al', '--anneal_lr', type=bool, help='How to anneal our learning rate', default=True)
+	parser.add_argument('-al', '--anneal_lr', type=lambda x: bool(strtobool(x)), help='How to anneal our learning rate', default=True, const=True)
 	parser.add_argument('-gl', '--gae_lambda', type=float, help="the lambda for the general advantage estimation", default=0.95)
 	parser.add_argument('-ue', '--num_update_epochs', type=int, help='The  number of update epochs for the policy', default=4)
 	parser.add_argument('-ne', '--num_envs', type=int, help='Number of environments to run in our vectorized setup', default=4)
@@ -39,7 +42,7 @@ if __name__=='__main__':
 	parser.add_argument('-ec', '--entropy_coeff', type=float, help='Coefficient for entropy', default=0.01)
 	parser.add_argument('-vf', '--value_coeff', type=float, help='Coefficient for values', default=0.5)
 	parser.add_argument('-cf', '--clip_coeff', type=float, help="the surrogate clipping coefficient",  default=0.2)
-	parser.add_argument('-cvl', '--clip_vloss', type=bool, help="Clip the value loss", default=True)
+	parser.add_argument('-cvl', '--clip_vloss', type=lambda x: bool(strtobool(x)), help="Clip the value loss", default=True. nargs='?', const=True)
 	parser.add_argument('-mgn', '--max_grad_norm', type=float, help='the maximum norm for the gradient clipping', default=0.5)
 	parser.add_argument('-tkl', '--target_kl',type=float, help='The KL divergence that we will not exceed', default=None)
 	parser.add_argument('-na', '--norm_adv', type=bool, help='Normalize advantage estimates', default=True)
diff --git a/src/wandb/latest-run b/src/wandb/latest-run
index 09416ac..9662554 120000
--- a/src/wandb/latest-run
+++ b/src/wandb/latest-run
@@ -1 +1 @@
-run-20231206_015919-tc7tc7hh
\ No newline at end of file
+run-20231206_220241-267si70s
\ No newline at end of file
