diff --git a/src/actor_critic_2.pt b/src/actor_critic_2.pt
index 81defd1..f9a4e50 100644
Binary files a/src/actor_critic_2.pt and b/src/actor_critic_2.pt differ
diff --git a/src/alt.py b/src/alt.py
index 06637a5..770f63a 100644
--- a/src/alt.py
+++ b/src/alt.py
@@ -18,7 +18,7 @@ def parse_args():
     parser = argparse.ArgumentParser()
     parser.add_argument("--exp-name", type=str, default=os.path.basename(__file__).rstrip(".py"),
         help="the name of this experiment")
-    parser.add_argument("--gym-id", type=str, default="CartPole-v1",
+    parser.add_argument("--gym-id", type=str, default="LunarLander-v2",
         help="the id of the gym environment")
     parser.add_argument("--learning-rate", type=float, default=2.5e-4,
         help="the learning rate of the optimizer")
diff --git a/src/episodic lengths_num_layers_2_dropout_0.0_num_envs_4_num_mb_4.png b/src/episodic lengths_num_layers_2_dropout_0.0_num_envs_4_num_mb_4.png
index 7a30ed7..931148b 100644
Binary files a/src/episodic lengths_num_layers_2_dropout_0.0_num_envs_4_num_mb_4.png and b/src/episodic lengths_num_layers_2_dropout_0.0_num_envs_4_num_mb_4.png differ
diff --git a/src/episodic returns_num_layers_2_dropout_0.0_num_envs_4_num_mb_4.png b/src/episodic returns_num_layers_2_dropout_0.0_num_envs_4_num_mb_4.png
index 571acfa..c0309e2 100644
Binary files a/src/episodic returns_num_layers_2_dropout_0.0_num_envs_4_num_mb_4.png and b/src/episodic returns_num_layers_2_dropout_0.0_num_envs_4_num_mb_4.png differ
diff --git a/src/ppo.py b/src/ppo.py
index 1958599..dcf622c 100644
--- a/src/ppo.py
+++ b/src/ppo.py
@@ -14,12 +14,6 @@ from tqdm import tqdm
 import random
 
 device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
-#device = torch.device('cpu')
-seed = 1
-random.seed(seed)
-np.random.seed(seed)
-torch.manual_seed(seed)
-torch.backends.cudnn.deterministic = True 
 
 #print(device)
 class torch_buffer():
@@ -35,14 +29,13 @@ class torch_buffer():
 
 	# flatten the buffer values for evaluation
 	def flatten(self, returns, advantages):
-		with torch.no_grad():
-			b_obs = self.states.reshape((-1,) + self.observation_shape)
-			b_logprobs = self.log_probs.reshape(-1)
-			b_actions = self.actions.reshape((-1,) + self.action_shape)
-			b_advantages = advantages.reshape(-1)
-			b_returns = returns.reshape(-1)
-			b_values = self.values.reshape(-1)
-			return b_obs, b_logprobs, b_actions, b_advantages, b_returns, b_values
+		b_obs = self.states.reshape((-1,) + self.observation_shape)
+		b_logprobs = self.log_probs.reshape(-1)
+		b_actions = self.actions.reshape((-1,) + self.action_shape)
+		b_advantages = advantages.reshape(-1)
+		b_returns = returns.reshape(-1)
+		b_values = self.values.reshape(-1)
+		return b_obs, b_logprobs, b_actions, b_advantages, b_returns, b_values
 
 # implement their plotting strategies
 # weights and biases graphs
@@ -174,7 +167,6 @@ class ppo():
 
 	def advantages(self, next_obs, next_done):
 		with torch.no_grad():
-			#next_value = self.policy_old.critic(next_obs).reshape(1, -1)
 			next_value = self.policy_old.value(next_obs)
 			if self.gae:
 				returns, advantages = self.run_gae(next_value, next_done)
@@ -191,8 +183,14 @@ class ppo():
 		writer.add_text("what", "what")
 		writer.add_text(
         "hyperparameters",
-        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{self.params_dict[key]}|" for key in self.params_dict])),
+        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{str(self.params_dict[key])}|" for key in self.params_dict])),
     	)
+		seed = 1
+		random.seed(seed)
+		np.random.seed(seed)
+		torch.manual_seed(seed)
+		torch.backends.cudnn.deterministic = True 
+
 		global_step = 0
 		start_time = time.time()
 		next_obs = torch.Tensor(self.envs.reset(seed=list(range(self.num_envs)))[0]).to(device)
@@ -245,6 +243,7 @@ class ppo():
 					policy_losses.append(policy_loss.item())
 
 					# value clipping
+					newvalue = newvalue.view(-1)
 					if self.clip_vloss:
 						v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
 						v_clipped = b_values[mb_inds] + torch.clamp(
@@ -256,17 +255,20 @@ class ppo():
 						v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
 						value_loss = 0.5 * v_loss_max.mean()
 					else:
-						value_loss = 0.5 * (((newvalue.view(-1) - b_values[mb_inds])) ** 2).mean()
+						value_loss = 0.5 * (((newvalue - b_values[mb_inds])) ** 2).mean()
 
 					entropy_loss = entropy.mean()
 					loss = policy_loss - self.entropy_coeff * entropy_loss + value_loss * self.value_coeff
+
 					self.optimizer.zero_grad()
 					loss.backward()
 					nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
 					self.optimizer.step()
+
 				if self.target_kl is not None:
 					if approx_kl > self.target_kl:
 						break
+
 			self.policy_old.load_state_dict(self.policy.state_dict())
 			policy_losses.append(policy_loss.item())
 
@@ -313,7 +315,7 @@ class ppo():
 		smoothed_returns = self.moving_average(episodic_returns, window_size)
 		plt.plot(x_indices, episodic_returns, label='Episodic Returns')
 		plt.plot(x_indices[9:], smoothed_returns, label=f'Moving Average (Window Size = {window_size})', color='red')
-		plt.title('Episodic Returns with Moving Average for Cartpole Problem')
+		plt.title('Episodic Returns with Moving Average for ' + self.gym_id)
 		plt.xlabel('Timestep')
 		plt.ylabel('Return')
 		plt.legend()
diff --git a/src/run.py b/src/run.py
index 1e28cb6..2f5e700 100644
--- a/src/run.py
+++ b/src/run.py
@@ -26,11 +26,11 @@ def plot_curves(arr_list, legend_list, x_indices, color_list, ylabel, fig_title)
 
 if __name__=='__main__':
 	parser = argparse.ArgumentParser()
-	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='CartPole-v1')
+	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='LunarLander-v2')
 	parser.add_argument('-s', '--seed', type=float, help='Seed for experiment', default=1.0)
 	parser.add_argument('-ns', '--num_steps', type=int, help='Number of steps that the environment should take', default=128)
 	parser.add_argument('-gae', '--gae', type=bool, help='Generalized Advantage Estimation flag', default=True)
-	parser.add_argument('-t', '--total_timesteps', type=int, help='Total number of timesteps that we will take', default=500000)
+	parser.add_argument('-t', '--total_timesteps', type=int, help='Total number of timesteps that we will take', default=1500000)
 	parser.add_argument('-al', '--anneal_lr', type=bool, help='How to anneal our learning rate', default=True)
 	parser.add_argument('-gl', '--gae_lambda', type=float, help="the lambda for the general advantage estimation", default=0.95)
 	parser.add_argument('-ue', '--num_update_epochs', type=int, help='The  number of update epochs for the policy', default=4)
@@ -39,7 +39,7 @@ if __name__=='__main__':
 	parser.add_argument('-ec', '--entropy_coeff', type=float, help='Coefficient for entropy', default=0.01)
 	parser.add_argument('-vf', '--value_coeff', type=float, help='Coefficient for values', default=0.5)
 	parser.add_argument('-cf', '--clip_coeff', type=float, help="the surrogate clipping coefficient",  default=0.2)
-	parser.add_argument('-cvl', '--clip_vloss', type=bool, help="Clip the value loss", default=False)
+	parser.add_argument('-cvl', '--clip_vloss', type=bool, help="Clip the value loss", default=True)
 	parser.add_argument('-mgn', '--max_grad_norm', type=float, help='the maximum norm for the gradient clipping', default=0.5)
 	parser.add_argument('-tkl', '--target_kl',type=float, help='The KL divergence that we will not exceed', default=0.2)
 	parser.add_argument('-na', '--norm_adv', type=bool, help='Normalize advantage estimates', default=True)
diff --git a/src/wandb/latest-run b/src/wandb/latest-run
index 172d2d0..68e7658 120000
--- a/src/wandb/latest-run
+++ b/src/wandb/latest-run
@@ -1 +1 @@
-run-20231201_214234-ephpfqat
\ No newline at end of file
+run-20231202_131118-rsbtwjk7
\ No newline at end of file
