diff --git a/src/models/robot_actor_critic.py b/src/models/robot_actor_critic.py
index 1207755..428eb5c 100644
--- a/src/models/robot_actor_critic.py
+++ b/src/models/robot_actor_critic.py
@@ -126,7 +126,7 @@ class robot_actor_critic(nn.Module):
 		#action_mean = torch.tanh(action_mean)
 		entropy = dist.entropy()		
 		unscaled_actions, actions = self.decodeActions(*[action[:, i] for i in range(self.n_a)])
-		return actions, unscaled_actions, log_prob.sum(1), entropy.sum(1), self.critic(self.network(obs))
+		return actions, unscaled_actions, log_prob.sum(1), entropy.sum(1), self.critic(self.network(cat_obs))
 
 	# pretrain the actor alone
 	def evaluate_pretrain(self, state, obs, action=None):
@@ -145,3 +145,11 @@ class robot_actor_critic(nn.Module):
 		action_mean = torch.tanh(action_mean)
 		unscaled_action, action = self.decodeActions(*[action[:, i] for i in range(self.n_a)])
 		return action.to(torch.float16), unscaled_action.to(torch.float16)
+
+
+	def test_action(self, state, obs):
+		state_tile = state.reshape(state.size(0), 1, 1, 1).repeat(1, 1, obs.shape[2], obs.shape[3])
+		cat_obs = torch.cat([obs, state_tile], dim=1).to(self.device)
+		action_mean = self.actor(self.network(cat_obs))
+		mean = torch.tanh(action_mean)
+		return self.decodeActions(*[mean[:, i] for i in range(self.n_a)])
diff --git a/src/models/sac_core.py b/src/models/sac_core.py
index 6e45bf1..4d6933e 100644
--- a/src/models/sac_core.py
+++ b/src/models/sac_core.py
@@ -10,6 +10,12 @@ from torch.distributions.normal import Normal
 from nets.equiv import EquivariantActor, EquivariantCritic
 from nets.base_cnns import base_actor, base_critic, base_encoder
 
+def weights_init(m):
+    if isinstance(m, nn.Linear):
+        torch.nn.init.xavier_uniform_(m.weight, gain=1)
+        torch.nn.init.constant_(m.bias, 0)
+    elif isinstance(m, nn.Conv2d):
+        nn.init.xavier_normal_(m.weight.data)
 
 def combined_shape(length, shape=None):
     if shape is None:
@@ -32,16 +38,31 @@ LOG_STD_MIN = -20
 
 class SquashedGaussianMLPActor(nn.Module):
 
-    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, act_limit):
+    def __init__(self, hidden_sizes, activation, 
+        dx=0.02, 
+        dy=0.02, 
+        dz=0.02, 
+        dr=np.pi/8, 
+        n_a=5, 
+        tau=0.001,):
         super().__init__()
+
+        self.p_range = torch.tensor([0, 1])
+        self.dtheta_range = torch.tensor([-dr, dr])
+        self.dx_range = torch.tensor([-dx, dx])
+        self.dy_range = torch.tensor([-dy, dy])
+        self.dz_range = torch.tensor([-dz, dz])	
+        self.n_a = n_a
         #self.net = mlp([obs_dim] + list(hidden_sizes), activation, activation)
-        self.net = base_actor()
-        self.mu_layer = nn.Linear(hidden_sizes[-1], act_dim)
-        self.log_std_layer = nn.Linear(hidden_sizes[-1], act_dim)
-        self.act_limit = act_limit
+        
+        #
+        self.net = base_encoder()
+
+        # hidden_sizes
+        self.mu_layer = nn.Linear(hidden_sizes[-1], 5)
+        self.log_std_layer = nn.Linear(hidden_sizes[-1], 5)
+        #self.act_limit = act_limit
 
-    # changes to support robot
-    # courtesy of Dian Wang
     def decodeActions(self, *args):
         unscaled_p = args[0]
         unscaled_dx = args[1]
@@ -63,7 +84,7 @@ class SquashedGaussianMLPActor(nn.Module):
             unscaled_actions = torch.stack([unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz], dim=1)
         return unscaled_actions, actions
 
-	# scaled actions
+    # scaled actions
     def getActionFromPlan(self, plan):
         def getUnscaledAction(action, action_range):
             unscaled_action = 2 * (action - action_range[0]) / (action_range[1] - action_range[0]) - 1
@@ -82,12 +103,10 @@ class SquashedGaussianMLPActor(nn.Module):
             return self.decodeActions(unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz, unscaled_dtheta)
         else:
             return self.decodeActions(unscaled_p, unscaled_dx, unscaled_dy, unscaled_dz)
-        
 
-    def forward(self, state, obs, deterministic=False, with_logprob=True):
-        state_tile = state.reshape(state.size(0), 1, 1, 1).repeat(1, 1, obs.shape[2], obs.shape[3])
-        cat_obs = torch.cat([obs, state_tile], dim=1).to(self.device)
-        net_out = self.net(cat_obs)
+    def forward(self, obs, deterministic=False, with_logprob=True):
+        net_out = self.net(obs)
+
         mu = self.mu_layer(net_out)
         log_std = self.log_std_layer(net_out)
         log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)
@@ -113,43 +132,50 @@ class SquashedGaussianMLPActor(nn.Module):
             logp_pi = None
 
         pi_action = torch.tanh(pi_action)
-        pi_action = self.act_limit * pi_action
-
-        return pi_action, logp_pi
+        #pi_action = self.act_limit * pi_action
 
+        unscaled, actions = self.decodeActions(*[pi_action[:, i] for i in range(self.n_a)])
+        return unscaled, actions, logp_pi
 
-# is this the critic
-# this is a value function
 class MLPQFunction(nn.Module):
-
     def __init__(self, obs_dim, act_dim, hidden_sizes, activation):
         super().__init__()
         self.q = mlp([obs_dim + act_dim] + list(hidden_sizes) + [1], activation)
-        
-    # so the q function is processing the action, observation, and state
+
+    # how does this q function actually work?
     def forward(self, obs, act):
         q = self.q(torch.cat([obs, act], dim=-1))
         return torch.squeeze(q, -1) # Critical to ensure q has right shape.
 
 
-# we need to add support for our CNNs
-class MLPActorCritic(nn.Module):
-
-    def __init__(self, observation_space, action_space, hidden_sizes=(256,256),
-                 activation=nn.ReLU):
+class SACCritic(nn.Module):
+    def __init__(self, obs_shape=(2, 128, 128), action_dim=5):
         super().__init__()
+        self.state_conv_1 = base_encoder(obs_shape=(2, 128, 128), out_dim=128)
+        self.critic_fc_1 = torch.nn.Sequential(
+            torch.nn.Linear(128 + action_dim, 128),
+            nn.ReLU(inplace=True),
+            torch.nn.Linear(128, 1)
+        )
+        self.apply(weights_init)
+
+    def forward(self, obs, act):
+        conv_out = self.state_conv_1(obs)
+        out_1 = self.critic_fc_1(torch.cat((conv_out, act), dim=1))
+        return out_1 
 
-        obs_dim = observation_space.shape[0]
-        act_dim = action_space.shape[0]
-        act_limit = action_space.high[0]
+class MLPActorCritic(nn.Module):
+    def __init__(self, n_a=5, hidden_sizes=(1024,1024), activation=nn.ReLU):
+        super().__init__()
 
         # build policy and value functions
-        self.pi = SquashedGaussianMLPActor(obs_dim, act_dim, hidden_sizes, activation, act_limit)
-        self.q1 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)
-        self.q2 = MLPQFunction(obs_dim, act_dim, hidden_sizes, activation)
+        self.pi = SquashedGaussianMLPActor(hidden_sizes, activation)
+        self.q1 = SACCritic()
+        self.q2 = SACCritic()
 
-    # add the observation in here
-    def act(self, obs, deterministic=False):
+    def act(self, state, obs, deterministic=False):
+        state_tile = state.reshape(state.size(0), 1, 1, 1).repeat(1, 1, obs.shape[2], obs.shape[3])
+        cat_obs = torch.cat([obs, state_tile], dim=1)
         with torch.no_grad():
-            a, _ = self.pi(obs, deterministic, False)
-            return a.numpy()
\ No newline at end of file
+            u_a, a, _ = self.pi(cat_obs, deterministic, False)
+            return u_a.cpu().numpy(), a.cpu().numpy()
\ No newline at end of file
diff --git a/src/robot_ppo.py b/src/robot_ppo.py
index d165a7e..d13f9bb 100644
--- a/src/robot_ppo.py
+++ b/src/robot_ppo.py
@@ -120,7 +120,7 @@ class robot_ppo():
 			'obs_size': 128, 
 			'fast_mode': True, 
 			'action_sequence': 'pxyzr', 
-			'render': True, 
+			'render': self.render, 
 			'num_objects': 2, 
 			'random_orientation': True, 
 			'robot': 'kuka', 
@@ -173,7 +173,6 @@ class robot_ppo():
 			_, true_action, _, _, _ = self.expert.evaluate(next_state.to(device), next_obs.to(device))
 		self.buffer.true_actions[step] = true_action 
 		next_states, next_obs, reward, done = self.envs.step(actions, auto_reset=True)
-		print('next state', next_states.shape)
 		for i, rew in enumerate(reward):
 			self.episodic_returns.add_value(i, rew)
 		self.buffer.rewards[step] = reward.view(-1)
@@ -287,6 +286,22 @@ class robot_ppo():
 				nn.utils.clip_grad_norm_(self.expert.actor.parameters(), self.max_grad_norm)
 				self.pretrain_optimizer.step()
 
+	def test_env(self, writer):
+		with torch.no_grad():
+			num_eval_steps=1000
+			state, obs = self.envs.reset()
+			for step in range(num_eval_steps):
+				# do we have auto_reset = true?
+				#u_a, a = self.expert.test_action(state.to(device), obs.to(device))
+				scaled_agent, unscaled_agent, logprob, _, value = self.expert.evaluate(state.to(device), obs.to(device))
+				state, obs, reward, done = self.envs.step(scaled_agent, auto_reset=True)
+				for i, d in enumerate(done):
+					if d:
+						discounted_return, episode_length = self.episodic_returns.calc_discounted_return(i)
+						print('Episode length', episode_length)
+						writer.add_scalar("charts/test_discounted_episodic_return", discounted_return, step)
+						writer.add_scalar("charts/test_episodic_length", episode_length, step)
+
 	def update(self, buffer, update_epochs, batch_size, minibatch_size, policy_losses):
 		assert minibatch_size != 0
 
@@ -398,6 +413,8 @@ class robot_ppo():
 			self.pretrain_buffer.load_to_cpu()
 			print('Pretraining complete')
 
+		self.test_env(writer)
+
 		global_step = 0
 		start_time = time.time()
 		next_state, next_obs = self.envs.reset()
diff --git a/src/robot_run.py b/src/robot_run.py
index c257df8..76df6ba 100644
--- a/src/robot_run.py
+++ b/src/robot_run.py
@@ -8,6 +8,16 @@ import os, sys, argparse, time
 import matplotlib.pyplot as plt
 import numpy as np
 
+def str2bool(v):
+    if isinstance(v, bool):
+       return v
+    if v.lower() in ('yes', 'true', 't', 'y', '1'):
+        return True
+    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
+        return False
+    else:
+        raise argparse.ArgumentTypeError('Boolean value expected.')
+
 def plot_curves(arr_list, legend_list, x_indices, color_list, ylabel, fig_title):
 	plt.clf()
 	fig, ax = plt.subplots(figsize=(12, 8))
@@ -29,7 +39,7 @@ def plot_curves(arr_list, legend_list, x_indices, color_list, ylabel, fig_title)
 
 if __name__=='__main__':
 	parser = argparse.ArgumentParser()
-	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='close_loop_block_in_bowl')
+	parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='close_loop_block_picking')
 	parser.add_argument('-s', '--seed', type=float, help='Seed for experiment', default=1.0)
 	parser.add_argument('-gae', '--gae', type=bool, help='Generalized Advantage Estimation flag', default=True)
 
@@ -59,7 +69,7 @@ if __name__=='__main__':
 	parser.add_argument('-na', '--norm_adv', type=bool, help='Normalize advantage estimates', default=True)
 	parser.add_argument('-p', '--capture_video', type=bool, help='Whether to capture the video or not', default=False)
 	parser.add_argument('-d', '--hidden_dim', type=int, help='Hidden dimension of the neural networks in the actor critic', default=64)
-	parser.add_argument('-c', '--continuous', type=lambda x: bool(strtobool(x)), default=True, nargs='?', const=True)
+	parser.add_argument('-c', '--continuous', type=str2bool, default=True, nargs='?', const=True)
 	parser.add_argument('-exp', '--exp_name', type=str, help='Experiment name', default='close_loop_block_pulling')
 	parser.add_argument('-nl', '--num_layers', type=int, help='The number of layers in our actor and critic', default=2)
 	parser.add_argument('-do', '--dropout', type=float, help='Dropout in our actor and critic', default=0.0)
@@ -69,6 +79,8 @@ if __name__=='__main__':
 	parser.add_argument('-eq', '--equivariant', type=bool, help='Run the robot with equivariant networks, or not', default=False)
 	parser.add_argument('-anexp', '--anneal_exp', type=bool, help='Do we want to anneal the expert weight?', default=False)
 	parser.add_argument('-sfp', '--save_file_path', type=str, help='Where to save model to', default=None)
+	parser.add_argument('-render', '--render', type=str2bool, help='Whether or not to render the environment', default=False, nargs='?', const=False)
+	parser.add_argument('-dpr', '--do_pretraining', type=str2bool, help='Whether or not to render the environment', default=True, nargs='?', const=True)
 	args = parser.parse_args()
 
 	params = {
@@ -105,7 +117,9 @@ if __name__=='__main__':
 		'pretrain_batch_size':args.pretrain_batch_size,
 		'expert_weight':args.expert_weight,
 		'anneal_exp':args.anneal_exp,
-		'save_file_path':args.save_file_path
+		'save_file_path':args.save_file_path,
+        'render':args.render,
+        'do_pretraining':args.do_pretraining
 	}
 
 	#all_returns = []
diff --git a/src/sac.py b/src/sac.py
index bed905d..b725e21 100644
--- a/src/sac.py
+++ b/src/sac.py
@@ -1,21 +1,32 @@
-# I need a sanity check
-
+# sanity check
 # open ai SAC implementation, with a few changes
 
-
 from copy import deepcopy
 import itertools
 import numpy as np
 import torch
 from torch.optim import Adam
-import gym
 import time
-import aur_ppo.src.models.sac_core as core
+import sys
+
+sys.path.append('/work/nlp/b.irving/aur_ppo/src')
+import models.sac_core as core
+from utils.str2bool import str2bool
+from env_wrapper_2 import EnvWrapper
 
+from torch.utils.tensorboard import SummaryWriter
+from tqdm import tqdm
+import argparse
 
+# need to change in order to support parallel processing
+# as of now, this code supports one environment
+
+# add support for the different buffers
 class ReplayBuffer:
     """
     A simple FIFO experience replay buffer for SAC agents.
+
+    What are other experience replay buffers that could be used instead?
     """
 
     def __init__(self, obs_dim, act_dim, size):
@@ -29,7 +40,7 @@ class ReplayBuffer:
         self.ptr, self.size, self.max_size = 0, 0, size
 
     # how are the states of the grippers going to be stored?
-    def store(self, obs, state, act, rew, next_state, next_obs, done):
+    def store(self, obs, state, act, rew, next_obs, next_state, done):
         self.obs_buf[self.ptr] = obs
         self.obs2_buf[self.ptr] = next_obs
         self.states_buf_1[self.ptr] = state
@@ -41,7 +52,7 @@ class ReplayBuffer:
         self.size = min(self.size+1, self.max_size)
 
     def sample_batch(self, batch_size=32):
-        idxs = np.random.randint(0, self.size, size=batch_size)
+        idxs = np.random.randint(0, self.ptr, size=batch_size)
         batch = dict(obs=self.obs_buf[idxs],
                      obs2=self.obs2_buf[idxs],
                      states=self.states_buf_1[idxs],
@@ -50,13 +61,16 @@ class ReplayBuffer:
                      rew=self.rew_buf[idxs],
                      done=self.done_buf[idxs])
         return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}
+    
+    
 
 
 
-def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0, 
-        steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99, 
+def sac(envs, test_envs, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0, 
+        num_processes=1,steps_per_epoch=500, epochs=4, replay_size=int(1e6), gamma=0.99, 
         polyak=0.995, lr=1e-3, alpha=0.2, batch_size=100, start_steps=10000, 
-        update_after=1000, update_every=50, num_test_episodes=10, max_ep_len=1000, save_freq=1):
+        update_after=1000, update_every=50, pretrain_episodes=20, num_test_episodes=10, 
+        max_ep_len=1000,track=False, save_freq=1, gym_id=None, device=torch.device('cpu')):
     """
     Soft Actor-Critic (SAC)
 
@@ -155,19 +169,34 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
     """
 
 
+    if track:
+        import wandb
+        wandb.init(project='ppo',entity='Aurelian',sync_tensorboard=True,config=None,name=gym_id + '_' + str(lr)) #+ '_' 
+       # str(self.value_coeff) + '_' + str(self.entropy_coeff) + '_' + str(self.clip_coeff) + '_' + str(self.num_minibatches),monitor_gym=True,save_code=True)
+    writer = SummaryWriter(f"runs/{gym_id}")
+    writer.add_text("parameters/what", "what")
+    #writer.add_text(
+    #"hyperparameters",
+    #"|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{str(self.params_dict[key])}|" for key in self.params_dict])),
+    #)
+
+
     torch.manual_seed(seed)
     np.random.seed(seed)
 
-    env, test_env = env_fn(), env_fn()
-    obs_dim = env.observation_space.shape
-    act_dim = env.action_space.shape[0]
+    # will this make two copies
+    env, test_env = envs, test_envs
+    obs_dim = (1, 128, 128)
+    #act_dim = env.action_space.shape[0]
+    act_dim=5
 
     # Action limit for clamping: critically, assumes all dimensions share the same bound!
-    act_limit = env.action_space.high[0]
+    #act_limit = env.action_space.high[0]
 
     # Create actor-critic module and target networks
-    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)
-    ac_targ = deepcopy(ac)
+
+    ac = core.MLPActorCritic().to(device)
+    ac_targ = deepcopy(ac.to(device))
 
     # Freeze target networks with respect to optimizers (only update via polyak averaging)
     for p in ac_targ.parameters():
@@ -179,25 +208,43 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
     # Experience buffer
     replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)
 
+    pretrain_size = replay_size
+    pretrain_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=pretrain_size)
+
     # Count variables (protip: try to get a feel for how different size networks behave!)
     var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.q1, ac.q2])
 
     # Set up function for computing SAC Q-losses
     def compute_loss_q(data):
-        o, s, a, r, o2, s2, d = data['obs'], data['states'], data['act'], data['rew'], data['obs2'], data['states2'], data['done']
-
-        q1 = ac.q1(o,a)
-        q2 = ac.q2(o,a)
+        o, s, a, r, o2, s2, d = (torch.tensor(data['obs']).float(), 
+        torch.tensor(data['states']).float(), 
+        torch.tensor(data['act']).float().to(device), 
+        # should the reward be converted as well
+        torch.tensor(data['rew']).float().to(device), 
+        torch.tensor(data['obs2']).float(), 
+        torch.tensor(data['states2']).float(),
+        torch.tensor(data['done']).float().to(device))
+
+        # these will be numpy arrays
+        # so we can change them to tensors?
+        # concatenate for loss
+        s_tile = s.reshape(s.size(0), 1, 1, 1).repeat(1, 1, o.shape[2], o.shape[3])
+        cat_o = torch.cat([o, s_tile], dim=1).to(device)
+
+        s_tile_2 = s2.reshape(s2.size(0), 1, 1, 1).repeat(1, 1, o2.shape[2], o2.shape[3])
+        cat_o_2 = torch.cat([o2, s_tile_2], dim=1).to(device)
+        q1 = ac.q1(cat_o,a)
+        q2 = ac.q2(cat_o,a)
 
         # Bellman backup for Q functions
         with torch.no_grad():
             # Target actions come from *current* policy
-            a2, logp_a2 = ac.pi(o2)
+            u_a2, a2, logp_a2 = ac.pi(cat_o_2)
 
             # Target Q-values
-            q1_pi_targ = ac_targ.q1(o2, a2)
-            q2_pi_targ = ac_targ.q2(o2, a2)
-            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ)
+            q1_pi_targ = ac_targ.q1(cat_o_2, u_a2)
+            q2_pi_targ = ac_targ.q2(cat_o_2, u_a2)
+            q_pi_targ = torch.min(q1_pi_targ, q2_pi_targ).to(device)
             backup = r + gamma * (1 - d) * (q_pi_targ - alpha * logp_a2)
 
         # MSE loss against Bellman backup
@@ -206,24 +253,35 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
         loss_q = loss_q1 + loss_q2
 
         # Useful info for logging
-        q_info = dict(Q1Vals=q1.detach().numpy(),
-                      Q2Vals=q2.detach().numpy())
+        q_info = dict(Q1Vals=q1.cpu().detach().numpy(),
+                      Q2Vals=q2.cpu().detach().numpy())
 
         return loss_q, q_info
 
     # Set up function for computing SAC pi loss
     def compute_loss_pi(data):
-        o = data['obs']
-        pi, logp_pi = ac.pi(o)
-        q1_pi = ac.q1(o, pi)
-        q2_pi = ac.q2(o, pi)
+
+        o = torch.tensor(data['obs']).float()
+        s = torch.tensor(data['states']).float()
+
+        # that means we have to compute the cat obs manually
+        # convert to tensors before processing
+
+        s_tile = s.reshape(s.size(0), 1, 1, 1).repeat(1, 1, o.shape[2], o.shape[3])
+        cat_o = torch.cat([o, s_tile], dim=1).to(device)
+
+        # need to change this
+        u_pi, pi, logp_pi = ac.pi(cat_o)
+        q1_pi = ac.q1(cat_o, pi)
+        q2_pi = ac.q2(cat_o, pi)
+
         q_pi = torch.min(q1_pi, q2_pi)
 
         # Entropy-regularized policy loss
         loss_pi = (alpha * logp_pi - q_pi).mean()
 
         # Useful info for logging
-        pi_info = dict(LogPi=logp_pi.detach().numpy())
+        pi_info = dict(LogPi=logp_pi.cpu().detach().numpy())
 
         return loss_pi, pi_info
 
@@ -267,37 +325,109 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
                 p_targ.data.mul_(polyak)
                 p_targ.data.add_((1 - polyak) * p.data)
 
-    def get_action(o, deterministic=False):
-        return ac.act(torch.as_tensor(o, dtype=torch.float32), 
+    def get_action(s, o, deterministic=False):
+        return ac.act(torch.from_numpy(s).float().to(device), torch.from_numpy(o).float().to(device),
                       deterministic)
 
     def test_agent():
-        for j in range(num_test_episodes):
-            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0
+        for j in tqdm(range(num_test_episodes)):
+            #o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0
+            s, o = test_env.reset()
+            d, ep_ret, ep_len = False, 0, 0
             while not(d or (ep_len == max_ep_len)):
-                # Take deterministic actions at test time 
-                o, r, d, _ = test_env.step(get_action(o, True))
+                u_a, a = get_action(s, o, True)
+                s, o, r, d = test_env.step(a)
                 ep_ret += r
                 ep_len += 1
 
+    # following Dians code really closely for the pretrainin
+    counter = 0
+    if pretrain_episodes > 0:
+        planner_envs = env
+
+        planner_num_process = num_processes
+        j = 0
+        states, obs = planner_envs.reset()
+        s = 0
+
+        #if not no_bar:
+        planner_bar = tqdm(total=pretrain_episodes)
+        local_transitions = []
+        while j < pretrain_episodes:
+            plan_actions = planner_envs.getNextAction()
+            # it doesn't matter what agent we use
+            planner_actions_star_idx, planner_actions_star = ac.pi.getActionFromPlan(plan_actions)
+            states_, obs_, rewards, dones = planner_envs.step(planner_actions_star, auto_reset=True)
+            #steps_lefts = planner_envs.getStepLeft()
+
+            # then iterate through each environment
+            for i in range(planner_num_process):
+                transition = (states[i], obs[i], planner_actions_star_idx[i],
+                                              rewards[i], states_[i], obs_[i], dones[i])
+                                              #steps_lefts[i].numpy(), np.array(1))
+                local_transitions.append(transition)
+            states = deepcopy(states_)
+            obs = deepcopy(obs_)
+            if dones.sum() and rewards.sum():
+                for t in local_transitions:
+                    # store in the replay buffer
+                    # TODO: ADD TRANSITION TO REPLAY BUFFER
+                    # we should change the way that the replay buffer works though
+
+                    # we should have a replay buffer which is the correct size
+                    # for the pretrain episodes
+
+                    pretrain_buffer.store(t[1], t[0], t[2], t[3], t[5], t[4], t[6])
+
+                local_transitions = []
+                j += dones.sum().item()
+                s += rewards.sum().item()
+                #if not no_bar:
+                planner_bar.set_description('{:.3f}/{}, AVG: {:.3f}'.format(s, j, float(s) / j if j != 0 else 0))
+                planner_bar.update(dones.sum().item())
+
+            elif dones.sum():
+                local_transitions = []
+
+            counter += 1
+            # adding data augmentation and weighted buffer?
+            #if expert_aug_n > 0:
+            #    augmentBuffer(replay_buffer, buffer_aug_type, expert_aug_n)
+    print('Total size ', counter)
+    # separate method for the buffer
+    # how do we determine the number of pretraining steps
+    # versus the number of planning episodes?
+    # how to define pretraining batchsize
+    # however many local transitions we want to process I guess?
+    pretrain_step = counter // batch_size
+    if pretrain_step > 0:
+        pbar = tqdm(total=pretrain_step)
+        for i in range(pretrain_step):
+            batch = pretrain_buffer.sample_batch(batch_size)
+            update(data=batch)
+        
+
     # Prepare for interaction with environment
     total_steps = steps_per_epoch * epochs
+    print(total_steps)
+    print(epochs)
+    print(steps_per_epoch)
     start_time = time.time()
-    s, o, ep_ret, ep_len = env.reset(), 0, 0
+    s, o  = env.reset()
+    ep_ret, ep_len = 0, 0
 
     # Main loop: collect experience in env and update/log each epoch
-    for t in range(total_steps):
+    for t in tqdm(range(total_steps)):
         
+
+        # here, we can make some changes to the training loop
         # Until start_steps have elapsed, randomly sample actions
         # from a uniform distribution for better exploration. Afterwards, 
         # use the learned policy. 
-        if t > start_steps:
-            a = get_action(o)
-        else:
-            a = env.action_space.sample()
+        u_a, a = get_action(s, o)
 
         # Step the env
-        s2, o2, r, d, _ = env.step(a)
+        s2, o2, r, d  = env.step(a, auto_reset=True)
         ep_ret += r
         ep_len += 1
 
@@ -307,15 +437,18 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
         d = False if ep_len==max_ep_len else d
 
         # Store experience to replay buffer
-        replay_buffer.store(o, s, a, r, o2, d)
+        replay_buffer.store(o, s, u_a, r, o2, s2, d)
 
         # Super critical, easy to overlook step: make sure to update 
         # most recent observation!
         o = o2
+        # update the state as well?
+        s = s2
 
         # End of trajectory handling
         if d or (ep_len == max_ep_len):
-            o, ep_ret, ep_len = env.reset(), 0, 0
+            s, o = env.reset()
+            ep_ret, ep_len = 0,0
 
         # Update handling
         if t >= update_after and t % update_every == 0:
@@ -334,21 +467,52 @@ def sac(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0,
             test_agent()
 
             # Log info about epoch
+            # here, we add the episode reward and the global timestep
+            writer.add_scalar("charts/discounted_episodic_return", ep_ret, t)
+            writer.add_scalar("charts/episodic_length", ep_len, t)
 
 if __name__ == '__main__':
-    import argparse
     parser = argparse.ArgumentParser()
     #parser.add_argument('--env', type=str, default='HalfCheetah-v2')
-    parser.add_argument('--hid', type=int, default=256)
+    parser.add_argument('-id', '--gym_id', type=str, help='Id of the environment that we will use', default='close_loop_block_in_bowl')
+    parser.add_argument('--hid', type=int, default=1024)
     parser.add_argument('--l', type=int, default=2)
     parser.add_argument('--gamma', type=float, default=0.99)
     parser.add_argument('--seed', '-s', type=int, default=0)
     parser.add_argument('--epochs', type=int, default=50)
     parser.add_argument('--exp_name', type=str, default='sac')
+    parser.add_argument('-tr', '--track', type=str2bool, help='Track the performance of the environment', nargs='?', const=False, default=False)
+    parser.add_argument('-ne', '--num_envs', type=int, default=1)
+    parser.add_argument('-re', '--render', type=str2bool, nargs='?', const=False, default=False)
     args = parser.parse_args()
 
+    simulator='pybullet'
+    env_config={'workspace': np.array([[ 0.25,  0.65],
+            [-0.2 ,  0.2 ],
+            [ 0.01,  0.25]]), 'max_steps': 100, 
+            'obs_size': 128, 
+            'fast_mode': True, 
+            'action_sequence': 'pxyzr', 
+            'render': args.render, 
+            'num_objects': 2, 
+            'random_orientation': True, 
+            'robot': 'kuka', 
+            'workspace_check': 'point', 
+            'object_scale_range': (1, 1), 
+            'hard_reset_freq': 1000, 
+            'physics_mode': 'fast', 
+            'view_type': 'camera_center_xyz', 
+            'obs_type': 'pixel', 
+            'view_scale': 1.5, 
+            'transparent_bin': True}
+    planner_config={'random_orientation': True, 'dpos': 0.02, 'drot': 0.19634954084936207}
+    envs = EnvWrapper(args.num_envs, simulator, args.gym_id, env_config, planner_config)
+    test_envs = EnvWrapper(args.num_envs, simulator, args.gym_id, env_config, planner_config)
     torch.set_num_threads(torch.get_num_threads())
 
-    sac(lambda : gym.make(args.env), actor_critic=core.MLPActorCritic,
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+
+    sac(envs, test_envs, actor_critic=core.MLPActorCritic,
         ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), 
-        gamma=args.gamma, seed=args.seed, epochs=args.epochs)
\ No newline at end of file
+        gamma=args.gamma, seed=args.seed, num_processes=args.num_envs, epochs=args.epochs,
+        track=args.track, gym_id=args.gym_id, device=device)
\ No newline at end of file
